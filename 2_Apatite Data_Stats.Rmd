---
title: "Apatite Data_Stats_v5"
author: "Spencer  Zeigler"
date: "6/15/2021"
output: html_document
editor_options:
  chunk_output_type: console
---

Double checking that these results don't overlap within 90% confidence:
```{r}
bootstrap.linreg(hex, "ft", df_name = "hex")
bootstrap.linreg(ellip , "ft", df_name = "ellip") 

pvalue <- sum(hex_slopes_volume < ellip_slopes_volume)/1001 
pvalue <- sum(hex_1_slopes_ft < hex_2_slopes_ft)/1001 
```


Calculating results according to termination:
```{r}
## Ft
bootstrap.linreg((hex %>% filter(np == "term0")), "ft", df_name = "hex_0")

names <- c("hex_0_results_ft", "hex_1_results_ft", "hex_2_results_ft", "hex_12_results_ft", "ellip_results_ft")
ft_results <- bind_rows(hex_0_results_ft, hex_1_results_ft, hex_2_results_ft, hex_12_results_ft, ellip_results_ft)
ft_results <- cbind(names, ft_results)

## Volume

bootstrap.linreg((hex %>% filter(np == "term0")), "volume", df_name = "hex_0")


names <- c("hex_0_results_volume", "hex_1_results_volume", "hex_2_results_volume", "hex_12_results_volume", "ellip_results_volume")
volume_results <- bind_rows(hex_0_results_volume, hex_1_results_volume, hex_2_results_volume, hex_12_results_volume, ellip_results_volume)
volume_results <- cbind(names, volume_results)
```

Double checking that these results don't overlap within 90% confidence:
```{r}
pvalue <- sum(hex_2_slopes_ft > hex_1_slopes_ft)/1001 
pvalue <- sum(hex_1_slopes_volume > hex_0_slopes_volume)/1001 
```

Uncertainty 
```{r}
# Ft
residual.uncertainty((filter(hex, np == "term0")), hex_0_slopes_ft, "ft", "hexagonal, 0 Np")
residual.uncertainty((filter(hex, np == "term1" | np == "term2")), hex_12_slopes_ft, "ft", "hexagonal, 1 or 2 Np")

residual.uncertainty(ellip, ellip_slopes_ft, "ft", "ellip")


# Volume
residual.uncertainty((filter(hex, np == "term0")), hex_0_slopes_volume, "volume", "hexagonal, 0 Np")
residual.uncertainty((filter(hex, np == "term1" | np == "term2")), hex_12_slopes_volume, "volume", "hexagonal, 1 or 2 Np")

#ggplotly(p, tooltip = c("key", "key2"))

residual.uncertainty(ellip, ellip_slopes_volume, "volume", "ellip")

```



# Nested Dataframe REQUIRED for all linear regressions below: 
```{r}
# Created nested dataframe. Can add parameters here, just change the "11" in "nrow" when compiling results into a dataframe. 

sample_df <- apatite %>% 
  select(gem, gc, ri, np, geo, size.cat, db.ft, s.ft, db.v, s.v, s.esr.ft, db.esr.ft) %>% 
  pivot_longer(1:6, values_to = "grouping") %>% 
  group_by(name, grouping) %>% 
  nest()

```

# Nested Bootstrapped Linear Regression:
## Forcing the Intercept Through 0
This code chunk creates the results (ie. mean slope, mean std.err, plot slope) & creates vectors of bootstrapped slopes for all categories

```{r}
# Use 'map' to apply function over a list of dataframes (ie. the nested df above). 
param <-  "volume" 
#MUST CHANGE THIS FOR EACH PARAMETER YOU WANT TO RUN #ie. "ft", "volume", "esr"

results_boot <- pmap(list(sample_df[[3]], 
                           glue("{param}")), 
                     bootstrap.linreg.nest)

# Convert the lists of lists spit out by 'map' into a dataframe that is pretty & in the correct order. 
# Seperate slopes from other results
results_flat <- flatten(results_boot) 
results_boot <- results_flat[seq(1, length(results_flat), 3)]
slopes_boot <- results_flat[seq(2, length(results_flat), 3)]
residuals_og <- results_flat[seq(3, length(results_flat), 3)]

# results --> tidy dataframe
results_boot_df <- data.frame(matrix(unlist(results_boot), nrow = 19, byrow = T))  %>% 
  rename(slope = X1, std.err = X2, plot.slope = X3) %>%
  bind_cols(sample_df[,2]) %>%
  relocate(grouping, .before = slope) %>%
  tibble() %>%
  mutate(grouping = factor(grouping, levels = c("A", "A1", "A2", "B", "B1", "B2", "C", "C1", "C2", "hexagonal", "ellipsoid", "1", "2", "term0", "term1", "term2", "rare- small", "common", "rare- large"))) %>%
  #mutate(grouping = factor(grouping, levels = c("A", "A1", "A2", "B", "B1", "B2", "hexagonal", "1", "2", "term0", "term1", "term2", "rare- small", "common", "rare- large"))) %>%
  arrange(grouping)

#Save results to named dataframe 
assign(paste("results_boot", glue("{param}"), sep =  "_"), results_boot_df)

# slopes --> tidy dataframe
# To calculate an uncertainty on the correction, a vector of all 1001 slopes split out by the bootstrapping procedure is saved into its own file.
slopes_boot_df <- data.frame(matrix(unlist(slopes_boot), nrow = 1001, byrow = FALSE)) %>%
  rename(C2 = X1, C = X2, `2` = X3, term1 = X4, ellipsoid = X5, common = X6, term0 = X7, C1 = X8, `1` = X9, term2 = X10, `rare- large` = X11, B2 = X12, B = X13, hexagonal = X14, B1 = X15, A2 = X16, A = X17, A1 = X18, `rare- small` = X19) %>%
  tibble() %>%
  select("A", "A1", "A2", "B", "B1", "B2", "C", "C1", "C2", "hexagonal", "ellipsoid", "1", "2", "term0", "term1", "term2", "rare- small", "common", "rare- large")

#Save results to named dataframe 
assign(paste("slopes_boot", glue("{param}"), sep =  "_"), slopes_boot_df)
```

Save all the outputs you just made to excel files so you don't have to run this slow-ass code again. 
```{r}
filename <- "Regression Results_Final.xlsx"

###### Bootstrapped results,intercept fixed at zero
xlsx::write.xlsx(as.data.frame(results_boot_ft), file = glue("{filename}"), sheetName="results_boot_ft", append = TRUE, row.names=FALSE)

xlsx::write.xlsx(as.data.frame(results_boot_volume), file = glue("{filename}"), sheetName= "results_boot_volume", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(results_boot_esr), file=glue("{filename}"), sheetName="results_boot_esr", row.names=FALSE, append = TRUE)

###### Slopes, bootstrapped, intercept fixed at zero
xlsx::write.xlsx(as.data.frame(slopes_boot_ft), file=glue("{filename}"), sheetName="slopes_boot_ft", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(slopes_boot_volume), file=glue("{filename}"), sheetName="slopes_boot_volume", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(slopes_boot_esr), file=glue("{filename}"), sheetName="slopes_boot_esr", row.names=FALSE, append = TRUE)
```



# Bootstrap p-value comparison code: 

Groupings of interest for comparison:
```{r}
grouping_gem <- c("A1", "A2", "A", "B1", "B2", "B", "C1", "C2", "C")
grouping_geo <- c("hexagonal", "ellipsoid")
# grouping_ri <- c("1", "2")
# grouping_term <- c("term0", "term1", "term2")
# grouping_size <- c("rare- small", "common", "rare- large")
# grouping_all <- c("A1", "A2", "A", "B1", "B2", "B", "C1", "C2", "C", "hexagonal", "ellipsoid", "1", "2", "term0", "term1", "term2", "rare- small", "common", "rare- large")
```

Calculate p-value to compare GEM against each other:
```{r}
################ 90% +/- a few percent...
params <- "ft_ri"
grouping <- grouping_ri

manual.pvalue(slopes_boot_ft, 
              glue("{grouping}"), 
              glue("{params}"), 
              0.90, 0.1) 

```

Calculate a p-value to compare individual groups (ie. for hexagonal grains, compare Nps):
```{r}
#This is a dangerous manual game. Try to redo this code one day:
sum(slopes_boot[[4]] > slopes_boot[[9]])/1001
sum(slopes_boot1 > slopes_boot0)/1001
```

# Uncertainty Calculations

```{r}

residual.uncertainty <- function (parameter_df, slope_df, parameter, group) {
  #parameter_df --> a, b, c, ri1, ellip, etc. 
  #slope_df ---> hex_0_slopes_ft, etc. 
  #parameter --> "ft", "volume", "esr"
  #group --> "hexagonal", "term1", etc. (ie. the name of the grouping from the results_boot sheet)
  #color --> color the points in the output plot by something ie. size.cat, ri, etc. 
 
    sample_df <- parameter_df %>%
select(sample, gem, gc, ri, np, broken, size.cat, j.w1, s.ft, db.ft, s.v, db.v, db.esr.ft, s.esr.ft)
  
  if (parameter == "ft") {
    #slope <- results_boot_ft %>% filter(grouping == group) %>% select(slope) %>% as.numeric() 
    slope <- mean(slope_df)
    
    sample_df <- cbind(sample_df, slope)
  
    actual <- sample_df$s.ft
    yhat <- sample_df$slope * sample_df$db.ft
    resid <- actual - yhat 
    percent.diff <- (resid / actual) * 100

    sample_df <- cbind(percent.diff, sample_df)
  }
  
  if (parameter == "volume") {
    #slope <- results_boot_volume %>% filter(grouping == group) %>% select(slope) %>% as.numeric() 
    slope <- mean(slope_df)
    
    sample_df <- cbind(sample_df, slope)
  
    actual <- sample_df$s.v
    yhat <- sample_df$slope * sample_df$db.v
    resid <- actual - yhat 
    percent.diff <- (resid / actual) * 100

    sample_df <- cbind(percent.diff, sample_df)
    
  }
  
    if (parameter == "esr") {
    slope <- mean(slope_df)
  
    sample_df <- cbind(sample_df, slope)
  
    actual <- sample_df$s.esr.ft
    yhat <- sample_df$slope * sample_df$db.esr.ft
    resid <- actual - yhat 
    percent.diff <- (resid / actual) * 100

    sample_df <- cbind(percent.diff, sample_df)
    
  }
  
p <- ggplot(sample_df, aes(j.w1, percent.diff)) +
  geom_hline(mapping=aes(yintercept = 0)) +
  labs(x = "Maximum Width (Âµm)", y = "Residuals as a % Difference", title = paste(glue("{parameter}"), "residuals for", glue("{group}"), "grains", sep = " ")) 

assign(paste("uncertainty", glue("{parameter}"), glue("{group}"), sep = "_"), (sd(percent.diff)), envir = parent.frame()) 

ri_summary <- sample_df %>%
  select(sample, ri, size.cat, gc, gem, np, percent.diff) %>%
  group_by(ri) %>%
  summarise(sd_residuals = sd(percent.diff))

size_summary <- sample_df %>%
  select(sample, ri, size.cat, gc, gem, np, percent.diff) %>%
  group_by(size.cat) %>%
  summarise(sd_residuals = sd(percent.diff))

print(ri_summary)
print(size_summary)
print(tibble(sd(percent.diff)))

if (parameter == "volume" & group != "ellip") {
  p <- p + geom_point(mapping= aes(color = ri)) + scale_color_uchicago(palette = "default")
} 
if (parameter == "ft" & group != "ellip") { 
  p <- p + geom_point(mapping = aes(color = size.cat)) + scale_color_jco(palette = "default")
}

if (group == "ellip") { #parameter == "ft" | parameter == "volume" & 
  p <- p + geom_point(color = "darkseagreen4") 
}

  return(p)
}
```




################################################################################
Bootstrapping Code 

######### INTERCEPT NOT FORCED THROUGH ZERO 

# Nested Bootstrapped Linear Regression:
## NOT Forcing the Intercept Through 0
This code chunk creates the results (ie. mean slope, mean std.err, plot slope) and creates vectors of bootstrapped intercepts for all categories
```{r}
param <- "volume" #MUST CHANGE THIS FOR EACH PARAMETER YOU WANT TO RUN #ie. "ft", "volume", "esr"

results_nf_boot <- pmap(list(sample_df[[3]], 
                          glue("{param}")), 
                     bootstrap.linreg.nest.not.fixed)

# Convert the lists of lists spit out by 'map' into a dataframe that is pretty & in the correct order. 
# Separate slopes from other results
results_nf_flat <- flatten(results_nf_boot) 
results_nf_boot <- results_nf_flat[seq(1, length(results_nf_flat), 3)]
intercepts_boot <- results_nf_flat[seq(2, length(results_nf_flat), 3)]
slopes_nf_boot <- results_nf_flat[seq(3, length(results_nf_flat), 3)]
  
results_nf_boot_df <- data.frame(matrix(unlist(results_nf_boot), nrow = 19, byrow = T)) %>%
  rename(slope = X1, intercept = X2, std.err = X3, plot.slope = X4, lower.conf.intercept = X5, upper.conf.intercept = X6) %>%
  bind_cols(sample_df[,2]) %>%
  relocate(grouping, .before = slope.mean) %>%
  tibble() %>%
  mutate(grouping = factor(grouping, levels = c("A", "A1", "A2", "B", "B1", "B2", "C", "C1", "C2", "hexagonal", "ellipsoid", "1", "2", "term0", "term1", "term2", "rare- small", "common", "rare- large"))) %>%
  arrange(grouping)

#Save results to named dataframe 
assign(paste("results_nf_boot", glue("{param}"), sep =  "_"), results_nf_boot_df)

# intercepts --> tidy dataframe
# To calculate an uncertainty on the correction, a vector of all 1001 intercepts split out by the bootstrapping procedure is saved into its own file.

intercepts_boot_df <- data.frame(matrix(unlist(intercepts_boot), nrow = 1001, byrow = FALSE)) %>%
  rename(C2 = X1, C = X2, `2` = X3, term1 = X4, ellipsoid = X5, common = X6, term0 = X7, C1 = X8, `1` = X9, term2 = X10, `rare- large` = X11, B2 = X12, B = X13, hexagonal = X14, B1 = X15, A2 = X16, A = X17, A1 = X18, `rare- small` = X19) %>%
  tibble() %>%
  select("A", "A1", "A2", "B", "B1", "B2", "C", "C1", "C2", "hexagonal", "ellipsoid", "1", "2", "term0", "term1", "term2", "rare- small", "common", "rare- large")

#Save results to named dataframe 
assign(paste("intercepts_boot", glue("{param}"), sep =  "_"), intercepts_boot_df)

# slopes --> tidy dataframe
# To calculate an uncertainty on the correction, a vector of all 1001 slopes split out by the bootstrapping procedure is saved into its own file.

slopes_nf_boot_df <- data.frame(matrix(unlist(slopes_nf_boot), nrow = 1001, byrow = FALSE)) %>%
  rename(C2 = X1, C = X2, `2` = X3, term1 = X4, ellipsoid = X5, common = X6, term0 = X7, C1 = X8, `1` = X9, term2 = X10, `rare- large` = X11, B2 = X12, B = X13, hexagonal = X14, B1 = X15, A2 = X16, A = X17, A1 = X18, `rare- small` = X19) %>%
  tibble() %>%
  select("A", "A1", "A2", "B", "B1", "B2", "C", "C1", "C2", "hexagonal", "ellipsoid", "1", "2", "term0", "term1", "term2", "rare- small", "common", "rare- large")

#Save results to named dataframe 
assign(paste("slopes_nf_boot", glue("{param}"), sep =  "_"), slopes_nf_boot_df)
```

Save all the outputs you just made to excel files so you don't have to run this slow-ass code again. 
```{r}
##### Bootstrapped results,intercept not fixed at zero

xlsx::write.xlsx(as.data.frame(results_nf_boot_ft), file="Regression Results_Final.xlsx", sheetName="results_nf_boot_ft", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(results_nf_boot_volume), file="Regression Results_Final.xlsx", sheetName="results_nf_boot_volume", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(results_nf_boot_esr), file="Regression Results_Final.xlsx", sheetName="results_nf_boot_esr", row.names=FALSE, append = TRUE)

###### Intercepts, bootstrapped, intercept not fixed at zero

xlsx::write.xlsx(as.data.frame(intercepts_boot_ft), file="Regression Results_Final.xlsx", sheetName="intercepts_boot_ft", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(intercepts_boot_volume), file="Regression Results_Final.xlsx", sheetName="intercepts_boot_volume", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(intercepts_boot_esr), file="Regression Results_Final.xlsx", sheetName="intercepts_boot_esr", row.names=FALSE, append = TRUE)

###### Slopes, bootstrapped, intercept not fixed at zero

xlsx::write.xlsx(as.data.frame(slopes_nf_boot_ft), file="Regression Results_Final.xlsx", sheetName="slopes_nf_boot_ft", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(slopes_nf_boot_volume), file="Regression Results_Final.xlsx", sheetName="slopes_nf_boot_volume", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(slopes_nf_boot_esr), file="Regression Results_Final.xlsx", sheetName="slopes_nf_boot_esr", row.names=FALSE, append = TRUE)
```


################################################################################
Taylor Code 

# Taylor Linear Regression:
## Forcing the Intercept Through 0
This code chunk creates the results (ie. slope, sigma slope, plot slope)
```{r}
param <- "esr"

results_taylor <- pmap(list(sample_df[[3]], 
                            glue("{param}")), 
                       taylor.uncertainty)

results_taylor_df <- data.frame(matrix(unlist(results_taylor), nrow = 19, byrow = TRUE)) %>%
  rename(slope = X1, sigma.slope = X2, plot.slope = X3) %>%
  bind_cols(sample_df[,2], sample_df[,1]) %>%
  relocate(grouping, name, .before = slope) %>%
  tibble() %>%
  mutate(grouping = factor(grouping, levels = c("A", "A1", "A2", "B", "B1", "B2", "C", "C1", "C2", "hexagonal", "ellipsoid", "1", "2", "term0", "term1", "term2", "rare- small", "common", "rare- large"))) %>%
  arrange(grouping)

#Save results to named dataframe 
assign(paste("results_taylor", glue("{param}"), sep =  "_"), results_taylor_df)
```

Save all the outputs you just made to excel files so you don't have to run this slow-ass code again. 
```{r}
filename <- "Regression Results_Final.xlsx"
#glue("{filename}")
###### Taylor results, intercept fixed at zero

xlsx::write.xlsx(as.data.frame(results_taylor_ft), file= glue("{filename}"), sheetName="results_taylor_ft", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(results_taylor_volume), file = glue("{filename}"), sheetName="results_taylor_volume", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(results_taylor_esr), file = glue("{filename}"), sheetName="results_taylor_esr", row.names=FALSE, append = TRUE)
```

# Taylor Linear Regression:
## NOT Forcing the Intercept Through 0
This code chunk creates the results (ie. slope, intercept, sigma slope, sigma intercept...)
```{r}
param <- "esr"
results_nf_taylor <- pmap(list(sample_df[[3]], 
                            glue("{param}")), 
                       taylor.uncertainty.not.fixed)

results_nf_taylor_df <- data.frame(matrix(unlist(results_nf_taylor), nrow = 19, byrow = TRUE)) %>%
  rename(slope = X1, sigma.slope = X2, intercept = X3, sigma.intercept =X4, plot.slope = X5) %>%
  bind_cols(sample_df[,2], sample_df[,1]) %>%
  relocate(grouping, name, .before = slope) %>%
  tibble() %>%
  mutate(grouping = factor(grouping, levels = c("A", "A1", "A2", "B", "B1", "B2", "C", "C1", "C2", "hexagonal", "ellipsoid", "1", "2", "term0", "term1", "term2", "rare- small", "common", "rare- large"))) %>%
  arrange(grouping)

#Save results to named dataframe 
assign(paste("results_nf_taylor", glue("{param}"), sep =  "_"), results_nf_taylor_df)
```

Save all the outputs you just made to excel files so you don't have to run this slow-ass code again. 
```{r}
filename <- "Regression Results_Final.xlsx"

###### Taylor results,intercept not fixed at zero

xlsx::write.xlsx(as.data.frame(results_nf_taylor_ft), file =  glue("{filename}"), sheetName="results_nf_taylor_ft", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(results_nf_taylor_volume), file =  glue("{filename}"), sheetName="results_nf_taylor_volume", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(results_nf_taylor_esr), file = glue("{filename}"), sheetName="results_nf_taylor_esr", row.names=FALSE, append = TRUE)
```

Eventually, the code to compare Taylor results easily to see if they overlap within 1sigma will exist: 

# Slope compare:
```{r}
params <- "volume_gem"
grouping <- grouping_gem

taylor.overlap(results_taylor_volume, 
               glue("{grouping}"), 
               glue("{params}"))
```

# Intercept compare:









