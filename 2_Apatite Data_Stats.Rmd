---
title: "Apatite Data_Stats_v5"
author: "Spencer  Zeigler"
date: "6/15/2021"
output: html_document
editor_options:
  chunk_output_type: console
---

#Uncertainty Calculations
```{r}
apatite <- apatite %>%
  mutate(ri.cat = ifelse(ri == "3", "3", "12"))

df <- apatite %>%
  select(sample, gem.geo, ri, ri.cat, size.cat, s.ft, db.ft, s.v, db.v) %>%
  filter(gem.geo == "C") %>%
  nest_by(size.cat, ri.cat) 

models <- df %>%
  mutate(model = list(lm(s.v ~ 0 + db.v, data = data)))

#create db which are lists of 3d values the same length of each of the model results (residuals): this is for calculating the percent difference 
db <- vector("list", nrow(df))

for (i in 1:nrow(df)) {
  db[[i]] <- unlist(df[[3]][[i]]$db.v)
}

#extract each residual result
sample_residuals <- vector("list", nrow(df))
for (i in 1:nrow(df)) {
  sample_residuals[[i]] <- unlist(models$model[[i]][[2]])
}

#get final results 
results <-  matrix(nrow = nrow(df), ncol = 2)
median <- vector("numeric", nrow(df))
std.dev <- vector("numeric", nrow(df))
n <- vector("numeric", nrow(df))

for (i in 1:nrow(df)) {
  x <- sample_residuals[[i]]
  y <- db[[i]]
  
  percent.diff  <-(x/y)*100
  median[i] <- median(percent.diff)
  std.dev[i] <-  sd(percent.diff)
  n[i] <- length(x)
  
  results <- cbind(median, std.dev, n)
}
```


```{r}
#need to combine ri 1/2
x  <- ab_all_residuals_ft
median_boot_res <- rowMeans(x)

y <- ab_all
db <- y %>% select(sample, db.ft, db.v,db.esr.ft) 

percent.diff <- (x/y$db.ft)*100

sd(percent.diff)
```



```{r}
set.seed(123)
bootstrap.linreg(c, "volume", "c")
```

Creating uncertainty dataframes: 
### Only bit that needs to change is x, z, y. Need to change this before plotting.
```{r}
x <- c_all_residuals_ft[,1001] #extract residuals calculated from observed data
z <- c_all$db.ft #extract 'real' value to calculate % difference
y <- c_all #store dataframe that corresponds to calculated residuals

#Combine all columns from above & parameters we care about (ie. size bin and name)
original_residuals <- bind_cols(og.residuals = x,
                                percent.diff = (x/z)*100,
                                size.name = y$size.name, 
                                #size.bin = y$size.bin, 
                                size.cat = y$size.cat,
                                ri = y$ri) 
#Store just the percent difference value for plotting later
percent.diff <- original_residuals$percent.diff

#Format original residuals dataframe 
original_residuals <- pivot_longer(original_residuals, cols = 3:5, names_to = "category") 
#Set as factors to force output tables into the right order 
original_residuals$value <- factor(original_residuals$value, levels = c("rare- small", "common", "rare- large", "Small & Rare", "Small & Common", "Typical & Common", "Large & Common", "Large & Rare", "1", "2", "3"))

all_res <- original_residuals %>% 
  mutate(sign = "total") %>%
  group_by(value, sign) %>%
  summarise(median_res = median(percent.diff), sd_res = sd(percent.diff))

sign_res <- original_residuals %>%
  mutate(sign = ifelse(percent.diff > 0, "positive", "negative")) %>%
  group_by(value, sign) %>% 
  summarise(median_res = median(percent.diff), sd_res = sd(percent.diff))

res_plotting <- bind_rows(all_res, sign_res) 
res_results <- res_plotting %>%
  pivot_wider(values_from = 3:4, names_from = sign)
```

## Double filter 
```{r}
slope.uncertainty(c, "ft")
```

# t-test to assess if slopes are statistically the same or not
```{r}
ttest_compare(b_all_slopes_volume, c_all_slopes_volume)

t.test(b_slopes_volume, a_slopes_volume, conf.level = 0.6827)
x <- sample(ab_slopes_volume, 100)
y <- sample(a_slopes_volume, 100)
t.test(x,y)
#if t-test p value is < 2.2e-16, result will be a p-value of 0
#https://github.com/tidymodels/broom/issues/227

```


```{r}
linreg <- lm(s.ft ~ 0 + db.ft, gca)

std.err <- sqrt(sum((gca$s.ft - linreg$fitted.values)^2) / (nrow(gca) - 2))
sb <- std.err / sqrt(sum((gca$db.ft - mean(gca$db.ft))^2))

#This gives me the same uncertainty as the excel LINEST and for Taylor 
#https://sites.chem.utoronto.ca/chemistry/coursenotes/analsci/stats/ErrRegr.html
```


```{r}
results.a <- slope.uncertainty(gca, "ft")
results.b <- slope.uncertainty(gcb, "ft")
results.c <- slope.uncertainty(gcc, "ft")

t.test(results.a$slope, results.b$slope)

# m1, m2: the sample means
# s1, s2: the sample standard deviations
# n1, n2: the same sizes
# m0: the null value for the difference in means to be tested for. Default is 0. 
# equal.variance: whether or not to assume equal variance. Default is FALSE. 
t.test2 <- function(m1,m2,s1,s2,n1,n2,m0=0,equal.variance=FALSE)
{
    if( equal.variance==FALSE ) 
    {
        se <- sqrt( (s1^2/n1) + (s2^2/n2) )
        # welch-satterthwaite df
        df <- ( (s1^2/n1 + s2^2/n2)^2 )/( (s1^2/n1)^2/(n1-1) + (s2^2/n2)^2/(n2-1) )
    } else
    {
        # pooled standard deviation, scaled by the sample sizes
        se <- sqrt( (1/n1 + 1/n2) * ((n1-1)*s1^2 + (n2-1)*s2^2)/(n1+n2-2) ) 
        df <- n1+n2-2
    }      
    t <- (m1-m2-m0)/se 
    dat <- c(m1-m2, se, t, 2*pt(-abs(t),df))    
    names(dat) <- c("Difference of means", "Std Error", "t", "p-value")
    return(dat) 
}

t.test2(results.a$slope, results.b$slope, results.a$sigma.slope, results.b$sigma.slope, length(gca$s.ft), length(gcb$s.ft))

t.test2(mean(a_slopes_ft), mean(b_slopes_ft), sd(a_slopes_ft), sd(b_slopes_ft), 1001, 1001)
#A and B are equal

t.test2(results.a$slope, results.c$slope, results.a$sigma.slope, results.c$sigma.slope, length(gca$s.ft), length(gcc$s.ft))

#A and C are different (reject the null)

t.test2(results.b$slope, results.c$slope, results.b$sigma.slope, results.c$sigma.slope, length(gcb$s.ft), length(gcc$s.ft))

#B and C are different
```



Using bootstrap code to get vectors of slopes to perform t-tests on:
```{r}
    sample_df <- gcc %>% select(sample, s.ft,  db.ft) %>% rename(twoD = s.ft,  threeD = db.ft)
  
  # Set set.seed a starting number to generate a sequence of random numbers so that we can get reproducible results
  set.seed(123)
  #Perform bootstrap 
  sample_boot <- bootstraps(sample_df,
                            times = 1000,
                            apparent = TRUE)
  
  #Run linear regression on each bootstrap
  sample_models <- sample_boot %>% 
    mutate(model = map(splits, ~ lm(twoD ~ 0 + threeD,
                                    data = .) ),
           coef_inf = map(model, tidy))
  #Get coefficients
  sample_coefs <- sample_models %>% 
    unnest(coef_inf)
  

  
  #Get confidence interval 
  percentile_intervals <- int_pctl(sample_models,
                                   coef_inf)
  
  #Store results
  results <<- as.data.frame(cbind(percentile_intervals$.estimate, sd(sample_coefs$estimate),mean(sample_coefs$std.error), (1/percentile_intervals$.estimate))) %>%
    rename(slope = V1, sd = V2, std.error = V3, plot.slope= V4)
  
```

```{r}
lower.ci <- percentile_intervals$.estimate - percentile_intervals$.lower
upper.ci <- percentile_intervals$.upper - percentile_intervals$.estimate 
average.ci <- (lower.ci + upper.ci)/2
```


#Using t test to see if slope values overlap within 1sigma. Same as quadrature. 
```{r}
#Stick this into the bootstrap.linreg code-- unnested. 
  slopes.gcb <- sample_coefs$estimate
  slopes.gca <- sample_coefs$estimate
  slopes.gcc <- sample_coefs$estimate

#then do t-tests if you hate urself but you don't need to the results are the same as quadrature. 

t.test(ab_slopes_ft, a_slopes_ft)

gca.2.5 <- quantile(slopes.gca, .025)
gca.97.5 <- quantile(slopes.gca,.975)

gcc.2.5 <- quantile(slopes.gcc, .025)
gcc.97.5 <-  quantile(slopes.gcc, .975)

gcb.2.5 <- quantile(slopes.gcb, .025)
gcb.97.5 <-  quantile(slopes.gcb, .975)

```

Average % error
```{r}
percent.diff <- function(df) {
   ftd <- round(mean(abs(df$s.ft - df$db.ft)/((df$s.ft + df$db.ft)/2) * 100),2)
   fts <- round(2*sd(abs(df$s.ft - df$db.ft)/((df$s.ft + df$db.ft)/2) * 100),2)
 
    vd <- round(mean(abs(df$s.v - df$db.v)/((df$s.v + df$db.v)/2) * 100),2)
    vs <- round(2*sd(abs(df$s.v - df$db.v)/((df$s.v + df$db.v)/2) * 100),2)
  
    sad <- round(mean(abs(df$s.sa - df$db.sa)/((df$s.sa + df$db.sa)/2) * 100), 2)
    sas <- round(2*sd(abs(df$s.sa - df$db.sa)/((df$s.sa + df$db.sa)/2) * 100),2)
    esrd <- round(mean(abs(df$s.esr.ft - df$db.esr.ft)/((df$s.esr.ft + df$db.esr.ft)/2) * 100), 2)
    esrs <- round(2*sd(abs(df$s.esr.ft - df$db.esr.ft)/((df$s.esr.ft + df$db.esr.ft)/2) * 100),2)
  
    abs.diff <- c(ftd, vd, sad, esrd)
    twosigma <- c(fts, vs, sas, esrs)
    
return(data.frame(abs.diff, twosigma, row.names = c("ft", "vol", "sa", "esr")))
}

ri12 <- apatite %>% filter(ri == "1" | ri == "2")

percent.diff(apatite)
percent.diff(gcc)
percent.diff(ab)
percent.diff(ri12)
percent.diff(ri3)
```

#Sampling a subset of the data (without replacement)
```{r}
#Sample size should be ~10% of population 

unique2d <- matrix(nrow = 20, ncol = 1000)
unique3d <- matrix(nrow = 20, ncol = 1000)
uniqueslopes <- list()


for (i in 1:1000) {
  unique2d[,i] <- sample(common_apatite$s.ft, size=20, replace = F)
  unique3d[,i] <- sample(common_apatite$db.ft, size=20, replace = F)
  uniqueslopes[i] <- summary(lm(unique2d[,i] ~ 0 + unique3d[,i]))$coefficients[[1]]
}

uniqueslopes <- unlist(uniqueslopes) 
uniqueslopes <- as.data.frame(uniqueslopes)

unique2d <- as.data.frame(unique2d) %>%
  rename(simulated1 = V1, simulated2 = V2, simulated3 = V3, simulated4= V4)
unique2d <- unique2d %>% pivot_longer(cols = 1:4)

ggplot() + 
  geom_histogram(uniqueslopes, mapping = aes(x = uniqueslopes), fill = "white", color = "black") +
  labs(title = "Slopes calculated from 1000 samples of n= 20 (without replacement)", x = "Slopes") +
  geom_vline(xintercept = mean(uniqueslopes$uniqueslopes), color = "blue", size =1.5) +
  annotate("text", x = 1.045, y = 95, label = glue("mean = {round(mean(uniqueslopes$uniqueslopes),3)}"), size = 5)

sd(uniqueslopes$uniqueslopes)

ggplot() + 
  geom_histogram(unique2d, mapping=aes(x=value)) +
  facet_wrap(~name) +
  xlab("2D Ft") 

ggplot() + 
  geom_histogram(common_apatite, mapping=aes(x=s.ft)) 
```

#Sampling with replacement
```{r}
replaced2d <- matrix(nrow = 20, ncol = 1000)
replaced3d <- matrix(nrow = 20, ncol = 1000)
replacedslopes_ab <- list()
replacedslopes_c <- list()
for (i in 1:1000) {
  replaced2d[,i] <- sample(ab$s.v, size=20, replace = T)
  replaced3d[,i] <- sample(ab$db.v, size=20, replace = T)
  replacedslopes_ab[i] <- summary(lm(replaced2d[,i] ~ 0 + replaced3d[,i]))$coefficients[[1]]
}
replacedslopes_ab <- unlist(replacedslopes_ab)
replacedslopes_c <- unlist(replacedslopes_c)
replacedslopes_ <- as.data.frame(replacedslopes)

replaced2d <- as.data.frame(replaced2d) %>%
  rename(simulated1 = V1, simulated2 = V2, simulated3 = V3, simulated4= V4)
replaced2d <- replaced2d %>% pivot_longer(cols = 1:4)

ggplot() + 
  geom_histogram(replacedslopes, mapping = aes(x = replacedslopes), fill = "white", color = "black") +
  labs(title = "Slopes calculated from 1000 samples of n= 200 (with replacement)", x = "Slopes") +
  geom_vline(xintercept = mean(replacedslopes$replacedslopes), color = "red", size =1.5) +
  annotate("text", x = 1.035, y = 90, label = glue("mean = {round(mean(replacedslopes$replacedslopes),3)}"), size = 5)

sd(replacedslopes$replacedslopes)


mean(replacedslopes$replacedslopes)
ggplot() + 
  geom_histogram(replaced2d, mapping=aes(x=value)) +
  facet_wrap(~name) + xlab("2D Ft")


ggplot() + 
  geom_histogram(replaced2d, mapping=aes(x= replaced2d))




```




##Resources:

* [Bootstrapping Code](https://towardsdatascience.com/bootstrap-sampling-in-r-a7bc9d3ca14a)
* [Confidence Intervals vs Std Deviation](https://stats.stackexchange.com/questions/151541/confidence-intervals-vs-standard-deviation/377634#:~:text=standard%20deviation,-confidence%2Dinterval%20standard&text=The%2095%25%20confidence%20interval%20gives,a%20range%20of%20~95%25.)
* [Overlapping Confidence Intervals](https://towardsdatascience.com/tutorial-for-using-confidence-intervals-bootstrapping-860ba716aef3)




##DEAD TO MEEEE

#Quadrature
```{r}
slope.uncertainty(c, "volume")
#read in spreadsheet of linear regression values
quad <- read_excel("./Linear Reg Results.xlsx", sheet="comp")

#function
error.propagation(quad, "common.only", "volume", "B", "C")
```

