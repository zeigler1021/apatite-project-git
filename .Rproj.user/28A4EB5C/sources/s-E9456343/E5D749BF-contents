---
title: "Apatite Data_Stats"
author: "Spencer  Zeigler"
date: "6/15/2021"
output: html_document
editor_options:
  chunk_output_type: console
---

#Below is testing/working code: 

Bootstrapping individual GEM values: 
```{r}
bootstrap.analysis <- function (df, threeD, twoD, df_name) {
  
  #Create sample df with only columns "twoD" and "threeD"
  sample_df <- df %>% select(threeD, twoD)
  colnames(sample_df) <- c(all_of("threeD"), "twoD")
 
  #Perform bootstrap 
  sample_boot <- bootstraps(sample_df,
                            times = 10000,
                            apparent = TRUE)
  #Extract only the analysis datasets (ie. the 'bootstrapped' set). Ignore the assessment set.
  resamples <- map(.x = sample_boot$splits, .f = analysis) 
  
  #Run linear regressions on the analysis sets, produces 10,000 models
  sample_models <<- resamples %>%
              map(~ lm(twoD ~ 0 + threeD, data = .x)) 

  #Extract the slopes only
  sample_models_flat <- flatten(sample_models)
  sample_slopes <- sample_models_flat[seq(1, length(sample_models_flat), 12)]
  sample_slopes <- unlist(sample_slopes) 
  sample_slopes <- tibble(sample_slopes) %>% rename(slopes = sample_slopes)
  
  #Extract the residuals 
  #sample_residuals <- sample_models_flat[seq(2, length(sample_models_flat), 12)]  #extract residuals
 
  #sample_residuals <- data.frame(matrix(unlist(sample_residuals), nrow = 225, byrow = FALSE))  #get into a dataframe, each col is a bootstrap
  
   # #Get confidence interval 
   # percentile_intervals <<- int_pctl(sample_models,
   #                                  coef_inf)
  
  #Results
  results_boot <- as.data.frame(cbind(mean(sample_slopes$slopes), (1/mean(sample_slopes$slopes)))) %>%
              rename(slope = V1, plot_slope= V2)
  
  #Save results df's to global environment
  assign(paste("results", glue("{df_name}"), sep = "_"), results_boot, envir = parent.frame())
  assign(paste("slopes", glue("{df_name}"), sep = "_"), sample_slopes, envir = parent.frame())
  #assign(paste("residuals", glue("{df_name}"), sep = "_"), sample_residuals, envir = parent.frame())

  
  
  #  #Get coefficients
  # sample_coefs <- sample_models %>% 
  #   unnest(coef_inf)
  # 
  # #Get confidence interval 
  # percentile_intervals <- int_pctl(sample_models,
  #                                  coef_inf)
}
```

#Corrections
```{r}
#Ft
bootstrap.analysis(hex, "db.ft", "s.ft", "hex_ft_2np")
bootstrap.analysis(ellip, "db.ft", "s.ft", "ellip_ft_2np")

#Volume
bootstrap.analysis(hex, "db.v", "s.v.ognp", "hex_vol_allnp")
bootstrap.analysis(ellip, "db.v", "s.v.ognp", "ellip_vol_allnp")

bootstrap.analysis(hex, "db.v", "s.v", "hex_vol_2np")
bootstrap.analysis(ellip, "db.v", "s.v", "ellip_vol_2np")

#Rs
bootstrap.analysis(hex, "db.rs", "s.rs", "hex_rs_2np")
bootstrap.analysis(ellip, "db.rs", "s.rs", "ellip_rs_2np")


suppressMessages(bind_cols(results_hex_ft_2np$slope, results_ellip_ft_2np$slope) %>% rename(hex = ...1, ellip = ...2))
suppressMessages(bind_cols(results_hex_rs_2np$slope, results_ellip_rs_2np$slope) %>% rename(hex = ...1, ellip = ...2))
suppressMessages(bind_cols(results_hex_vol_2np$slope, results_ellip_vol_2np$slope, results_hex_vol_allnp$slope, results_ellip_vol_allnp$slope) %>% rename(hex_2np = ...1, ellip_2np = ...2, hex_allnp = ...3, ellip_allnp = ...4))
```

#Uncertainties 
```{r}
# slopes_df, df, nest_by, threeD, twoD,  

slope <- as.numeric(mean(slopes_ellip_vol_allnp$slopes))

uncerts_ellip_vol_allnp <- cbind(select(ellip, sample, s.v.ognp, db.v, ri, size.cat, j.w1), slope) %>%
  #group_by(size.cat) %>%
  mutate(residual = (s.v.ognp - (slope * db.v)), 
         percent_diff = (((s.v.ognp - (slope * db.v))/s.v.ognp) * 100)) %>%
  summarize(resid_sd = sd(residual), 
            resid_se = sqrt(var(residual)/n()), 
            percentdiff_sd = sd(percent_diff), 
            percentdiff_se = sqrt(var(percent_diff)/n()), 
            res_stdev = sqrt(sum(residual)^2/(n()-2)), 
            res_stdev_percentdiff = sqrt(sum(percent_diff)^2/(n()-2))
            )

n <- nrow(sample_df)

residual <- (sample_df$s.ft - (sample_df$slope * sample_df$db.ft))
  stdev <- sd(residual)
  stder <- stdev/sqrt(n)

percent_diff <- (residual / sample_df$s.ft) * 100 
  stdev <- sd(percent_diff)
  stder <- stdev/sqrt(n)

stder(slopes_hex_ft_2np)
  
stder <- function(x) sqrt(var(x)/length(x))
stder_results <- unlist(map(residuals_hex_ft_2np, stder)) 
stder_results <- tibble(stder_results)

#make a histogram of the percent differences
sample_models_flat <- flatten(sample_models)
boot_sample <- sample_models_flat[seq(12, length(sample_models_flat), 12)]
boot_sample_flat <- flatten(boot_sample)
boot_sample <- boot_sample_flat[seq(1, length(boot_sample_flat), 2)]
boot_sample <- data.frame(matrix(unlist(boot_sample), nrow = 225, byrow = FALSE))  #get into a dataframe, each col is a bootstrap

percentdiff_df <- matrix(ncol = 10001, nrow = 225)
for (i in 1:10001) {
  percentdiff_df[,i] <- ((residuals_hex_ft_2np[,i] / boot_sample[,i]) * 100)
}
percentdiff_df <- as.data.frame(percentdiff_df)
#stder(percentdiff_df[,1])
percentdiff_results <- unlist(map(percentdiff_df, sd))
percentdiff_results <- as.data.frame(percentdiff_results)

hist(stder_results$stder_results)
hist(percentdiff_results$percentdiff_results)

residuals_hex_ft_2np
```


# Below outputs final results in a 'pretty' and organized way. Not good for testing and working. 

Notes: 
* All 'termination end' grains have been removed
* All terminations have been set to Np = 2 so that our 2D calculations assume ejection through those surfaces, just like they do for Blob3D 
* The intercept is being forced through 0 for these regressions 

Workflow: 
* Compute regressions for all parameters of interest (in this case, all GEM values)
* Double check slope uncertainties (by manually calculating a p-value) to confirm that A and B grains are hexagonal and  distinct from ellipsoid C grains within a ~90% confidence
* Determine value of the slope by bootstrapping the regression (2D ~ 0 + 3D)
* Determine the uncertainty on the correction by analyzing the residuals calculated from the mean of the bootstrapped slopes (ie. the correction). The residuals are calculated and then turned into a percent difference using the residual and the actual value. This normalizes the residuals. 
* At the end we have a correction (slope) Â± uncertainty (normalized residuals)

Wow! Years of work to end up with two frickin' numbers. God damn. 

Hard to believe. 

## Bootstrapping
 Goal: compute results for all GEM values
 # Nested Dataframe REQUIRED for all linear regressions below: 
```{r}
# Created nested dataframe.

sample_df <- apatite %>% 
  select(gem, gc, geo, db.ft, s.ft, db.v, s.v, s.esr.ft, db.esr.ft) %>% 
  pivot_longer(1:3, values_to = "grouping") %>% 
  group_by(name, grouping) %>% 
  nest()
```

# Nested Bootstrapped Linear Regression:
## Forcing the Intercept Through 0
This code chunk creates the results (ie. mean slope, mean std.err, plot slope) & creates vectors of bootstrapped slopes for all categories

```{r}
# Use 'map' to apply function over a list of dataframes (ie. the nested df above). 
param <-  "esr" 
#MUST CHANGE THIS FOR EACH PARAMETER YOU WANT TO RUN #ie. "ft", "volume", "esr"

results_boot <- pmap(list(sample_df[[3]], 
                           glue("{param}")), 
                     bootstrap.linreg.nest)

# Convert the lists of lists spit out by 'map' into a dataframe that is pretty & in the correct order. 
# Separate slopes from other results
results_flat <- flatten(results_boot) 
results_boot <- results_flat[seq(1, length(results_flat), 3)]
slopes_boot <- results_flat[seq(2, length(results_flat), 3)]
residuals_og <- results_flat[seq(3, length(results_flat), 3)]

# results --> tidy dataframe
results_boot_df <- data.frame(matrix(unlist(results_boot), nrow = 11, byrow = T))  %>% 
  rename(slope = X1, std.err = X2, plot.slope = X3) %>%
  bind_cols(sample_df[,2]) %>%
  relocate(grouping, .before = slope) %>%
  tibble() %>%
  mutate(grouping = factor(grouping, levels = c("A", "A1", "A2", "B", "B1", "B2", "C", "C1", "C2", "hexagonal", "ellipsoid"))) %>%
  arrange(grouping)

#Save results to named dataframe 
assign(paste("results_boot", glue("{param}"), sep =  "_"), results_boot_df)

# slopes --> tidy dataframe
# To calculate an uncertainty on the correction, a vector of all 1001 slopes split out by the bootstrapping procedure is saved into its own file.
slopes_boot_df <- data.frame(matrix(unlist(slopes_boot), nrow = 1001, byrow = FALSE)) %>%
  rename(B2 = X1, B = X2, hexagonal = X3, A1 = X4, A = X5, B1 = X6, A2 = X7, C2 = X8, C = X9, ellipsoid = X10, C1 = X11) %>%
  tibble() %>%
  select("A", "A1", "A2", "B", "B1", "B2", "C", "C1", "C2", "hexagonal", "ellipsoid")

#Save results to named dataframe 
assign(paste("slopes_boot", glue("{param}"), sep =  "_"), slopes_boot_df)
```

Save all the outputs you just made to excel files so you don't have to run this slow-ass code again. 
```{r}
filename <- "Regression Results_Final.xlsx"

###### Bootstrapped results,intercept fixed at zero
xlsx::write.xlsx(as.data.frame(results_boot_ft), file = glue("{filename}"), sheetName="results_boot_ft", append = TRUE, row.names=FALSE)

xlsx::write.xlsx(as.data.frame(results_boot_volume), file = glue("{filename}"), sheetName= "results_boot_volume", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(results_boot_esr), file=glue("{filename}"), sheetName="results_boot_esr", row.names=FALSE, append = TRUE)

###### Slopes, bootstrapped, intercept fixed at zero
xlsx::write.xlsx(as.data.frame(slopes_boot_ft), file=glue("{filename}"), sheetName="slopes_boot_ft", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(slopes_boot_volume), file=glue("{filename}"), sheetName="slopes_boot_volume", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(slopes_boot_esr), file=glue("{filename}"), sheetName="slopes_boot_esr", row.names=FALSE, append = TRUE)
```

Result: 
```{r bootstrap_gem_ft, fig.cap = "Table of regression slopes for each GEM value"}
#Ft
kable(results_boot_ft %>% 
  select(grouping, slope) %>% 
  pivot_longer(cols = grouping) %>% 
  select(-name) %>% 
  pivot_wider(names_from = value, values_from = slope))
```

```{r bootstrap_gem_ft, fig.cap = "Table of regression slopes for each GEM value"}
#Volume
kable(results_boot_volume %>% 
  select(grouping, slope) %>% 
  pivot_longer(cols = grouping) %>% 
  select(-name) %>% 
  pivot_wider(names_from = value, values_from = slope))
```

```{r bootstrap_gem_ft, fig.cap = "Table of regression slopes for each GEM value"}
#Rs
kable(results_boot_esr %>% 
  select(grouping, slope) %>% 
  pivot_longer(cols = grouping) %>% 
  select(-name) %>% 
  pivot_wider(names_from = value, values_from = slope))
```


## p-values 
Goal: compute p-values for all combinations of GEM values and determine which categories are significantly different or not. 

Groupings of interest for comparison:
```{r groupings}
grouping_gem <- c("A1", "A2", "A", "B1", "B2", "B", "C1", "C2", "C")
grouping_geo <- c("hexagonal", "ellipsoid")
```

```{r}
# Must run this code twice. Once with all the suffixes on grouping and params = gem and once with it = geo.

grouping <- grouping_gem
params_ft <- "ft_gem"
params_vol <- "volume_gem"
#params_esr <- "esr_geo"

manual.pvalue(slopes_boot_ft, 
              glue("{grouping}"), 
              glue("{params_ft}"), 
              0.90, 0.1) 


manual.pvalue(slopes_boot_volume, 
              glue("{grouping}"), 
              glue("{params_vol}"), 
              0.90, 0.1) 


# manual.pvalue(slopes_boot_esr, 
#               glue("{grouping}"), 
#               glue("{params_esr}"), 
#               0.90, 0.1) 


####To compare tet to normal 
sum(slope_boot_tet_ft$C > slope_boot_normal_ft$C)/1001

```

Result: A and B grains are not significantly different (p < 0.9). C grains are significantly different (p > 0.99)
```{r, echo=FALSE}
#Ft
kable(pvalue_ft_gem %>% 
        rename(`p-value` = pvalue) %>% 
        unite("GEM Compare", var1:var2, sep = " vs. ") %>% 
        relocate(`GEM Compare`, .before = `p-value`))


p.gem <- ggplot(pvalue_ft_gem, aes(x = var1, y = var2, fill = discrete)) + 
    geom_tile(color = "black") +
    theme_bw() +
    theme(axis.title = element_blank()) +
  theme(legend.position = "none") +
    geom_text(aes(label = round(pvalue,2)), size = 3, color = "black") +
    scale_fill_manual(values = tf_color,
                      breaks = c("Slopes are diff", "Slopes are same", "NA"),
                      labels = c("Slopes are different", "Slopes are same", "NA"), 
                      name = "Legend") 

p.geo <- ggplot(pvalue_ft_geo, aes(x = var1, y = var2, fill = discrete)) + 
    geom_tile(color = "black") +
    theme_bw() +
    theme(axis.title = element_blank()) +
  
    geom_text(aes(label = round(pvalue,2)), size = 3, color = "black") +
    scale_fill_manual(values = tf_color,
                      breaks = c("Slopes are diff", "Slopes are same", "NA"),
                      labels = c("Slopes are different", "Slopes are same", "NA"), 
                      name = "Legend") 


p.gem + p.geo + patchwork::plot_annotation(title = "Are the slopes of the regression for\neach GEM category significantly different or not? ", subtitle = "Ft Slopes at 90% Confidence")
```

```{r, echo = FALSE}
#Volume
kable(pvalue_volume_gem %>% 
        rename(`p-value` = pvalue) %>% 
        unite("GEM Compare", var1:var2, sep = " vs. ") %>% 
        relocate(`GEM Compare`, .before = `p-value`))

p.gem <- ggplot(pvalue_volume_gem, aes(x = var1, y = var2, fill = discrete)) + 
    geom_tile(color = "black") +
    theme_bw() +
    theme(axis.title = element_blank()) +
    theme(legend.position = "none") +
    geom_text(aes(label = round(pvalue,2)), size = 3, color = "black") +
    scale_fill_manual(values = tf_color,
                      breaks = c("Slopes are diff", "Slopes are same", "NA"),
                      labels = c("Slopes are different", "Slopes are same", "NA"), 
                      name = "Legend") 

p.geo <- ggplot(pvalue_volume_geo, aes(x = var1, y = var2, fill = discrete)) + 
    geom_tile(color = "black") +
    theme_bw() +
    theme(axis.title = element_blank()) +
    geom_text(aes(label = round(pvalue,2)), size = 3, color = "black") +
    scale_fill_manual(values = tf_color,
                      breaks = c("Slopes are diff", "Slopes are same", "NA"),
                      labels = c("Slopes are different", "Slopes are same", "NA"), 
                      name = "Legend") 
p.gem + p.geo + patchwork::plot_annotation(title = "Are the slopes of the regression for\neach GEM category significantly different or not? ", subtitle = "Volume Slopes at 90% Confidence")

```

```{r, echo = FALSE}
#Rs
kable(pvalue_esr_gem %>% 
        rename(`p-value` = pvalue) %>% 
        unite("GEM Compare", var1:var2, sep = " vs. ") %>% 
        relocate(`GEM Compare`, .before = `p-value`))

p.gem <- ggplot(pvalue_esr_gem, aes(x = var1, y = var2, fill = discrete)) + 
    geom_tile(color = "black") +
    theme_bw() +
    theme(axis.title = element_blank()) +
  theme(legend.position = "none") +
    geom_text(aes(label = round(pvalue,2)), size = 3, color = "black") +
    scale_fill_manual(values = tf_color,
                      breaks = c("Slopes are diff", "Slopes are same", "NA"),
                      labels = c("Slopes are different", "Slopes are same", "NA"), 
                      name = "Legend") 

p.geo <- ggplot(pvalue_esr_geo, aes(x = var1, y = var2, fill = discrete)) + 
    geom_tile(color = "black") +
    theme_bw() +
    theme(axis.title = element_blank()) +
  
    geom_text(aes(label = round(pvalue,2)), size = 3, color = "black") +
    scale_fill_manual(values = tf_color,
                      breaks = c("Slopes are diff", "Slopes are same", "NA"),
                      labels = c("Slopes are\ndifferent", "Slopes are same", "NA"), 
                      name = "Legend") 

p.gem + p.geo + patchwork::plot_annotation(title = "Are the slopes of the regression for\neach GEM category significantly different or not? ", subtitle = "Rs Slopes at 90% Confidence")
```


## Bootstrapping 
Goal: get the correction values we care about. We want a correction for all A & B grains (hexagonal) and a correction for all C grains (ellipsoid)
```{r}
#Must run this code 3 times. Once for "ft", "volume" "esr". Change it at "parameter". 
parameter <- "ft"

#Hexagonal Grains 
hex <- filter(apatite, gc == "A" | gc == "B")
bootstrap.linreg(hex, parameter, "hex")

#Ellipsoid Grains
ellip <- filter(apatite, gc == "C")
bootstrap.linreg(ellip, parameter, "ellip")

```

Result: 
```{r}
#Ft
kable(bind_rows(select(hex_results_ft, slope), select(ellip_results_ft,slope)) %>% bind_cols(c("Hexagonal", "Ellipsoid")) %>% rename(Geometry = ...2, Slope = slope) %>% relocate(Geometry), caption = "The correction (ie. slope of the regression) for each geometry subset for Ft. Hexagonal = A & B grains, Ellipsoid = C grains.")
#Volume
kable(bind_rows(select(hex_results_volume, slope), select(ellip_results_volume,slope)) %>% bind_cols(c("Hexagonal", "Ellipsoid")) %>% rename(Geometry = ...2, Slope = slope) %>% relocate(Geometry), caption = "The correction (ie. slope of the regression) for each geometry subset for Volume. Hexagonal = A & B grains, Ellipsoid = C grains.")
#Rs
kable(bind_rows(select(hex_results_esr, slope), select(ellip_results_esr,slope)) %>% bind_cols(c("Hexagonal", "Ellipsoid")) %>% rename(Geometry = ...2, Slope = slope) %>% relocate(Geometry), caption = "The correction (ie. slope of the regression) for each geometry subset for Rs. Hexagonal = A & B grains, Ellipsoid = C grains.")
```

## Uncertainties
Goal A: determine if there are parameters that control the uncertainty on the regression. We will analyze two parameters: roughness and size. Only for hexagonal grains. The C category is too small to split into further categories (n=37) so the uncertainty will be flat. 

```{r}
#Ft Hexagonal
residual.uncertainty("ft", "hex")
#Volume Hexagonal
residual.uncertainty("volume", "hex")
#Rs Hexagonal
#residual.uncertainty("esr", "hex")
```

Results:
```{r}
#Ft Hexagonal
plots_hex_ft[[1]] + plots_hex_ft[[2]] + plot_annotation(title = "Residuals for Hexagonal Grains for Ft", subtitle = "Is there a pattern within roughness or size?")
 
kable(table_hex_ft[[1]])
kable(table_hex_ft[[2]])

#Volume Hexagonal
plots_hex_volume[[1]] + plots_hex_volume[[2]] + plot_annotation(title = "Residuals for Hexagonal Grains for Volume", subtitle = "Is there a pattern within roughness or size?")
 
kable(table_hex_volume[[1]])
kable(table_hex_volume[[2]])

#ESR Hexagonal
# plots_hex_esr[[1]] + plots_hex_esr[[2]] + plot_annotation(title = "Residuals for Hexagonal Grains for Rs", subtitle = "Is there a pattern within roughness or size?")
#  
# kable(table_hex_esr[[1]])
# kable(table_hex_esr[[2]])
```
Our final results indicate that for Ft and Rs, there is a pattern with regard to size and for volume, there is a pattern with regards to roughness. Therefore, our final results reflect this. The uncertainties on the correction are controlled by roughness or size. 

Goal B: determine the uncertainty on the correction using the residuals from the linear regression and split by controlling parameter (ie. for volume, roughness. for Ft/Rs, size). 
```{r}
kable(table_hex_ft[[2]], caption = "Uncertainty on Ft correction split by size category for hexagonal grains.")
kable(table_hex_volume[[1]], caption = "Uncertainty on Volume correction split by grain roughness.")
kable(table_hex_esr[[2]], caption = "Uncertainty on Rs correction split by size category.")

#Ellipsoid results
residual.uncertainty("ft", "ellip")
residual.uncertainty("volume", "ellip")
residual.uncertainty("esr", "ellip")

kable(bind_cols(c(sd(uncert_result_ellip_ft$percent_diff), sd(uncert_result_ellip_volume$percent_diff), sd(uncert_result_ellip_esr$percent_diff)), c("Ft", "Volume", "Rs")) %>% rename(Uncertainty = ...1, Parameter = ...2) %>% relocate(Parameter, .before = Uncertainty), caption = "Uncertainty on corrections for ellipsoid grains. Due to small sample number (N=37), these uncertainties are not controlled by a parameter like roughness or size.")
```


Results: 
```{r}
#Print a table that shows the influence of secondary parameters
#Print a table that shows the final results 
```

## Results! 
Goal: we have corrected for the geometric uncertainty for Ft, volume (eU), and Rs! 
```{r}
#print corrections and results and celebrate good times come on
```




################ Post Thermo 2021 Testing

I want to test: 
Corrections for each isotope specific Ft (do they overlap with 238)?
- 232
- 235
- 147 

Bootstrapping: 
```{r}
sample_df <- hex %>% 
      #filter(np.ognp == "term1") %>%
      select(sample, gem, s.ft,  db.ft) %>% 
      rename(twoD = s.ft, threeD = db.ft) 


  set.seed(123)
  #Perform bootstrap 
  sample_boot <- bootstraps(sample_df,
                            times = 1000,
                            apparent = TRUE)
  
  #Run linear regression on each bootstrap
  sample_models <- sample_boot %>% 
    mutate(model = map(splits, ~ lm(twoD ~ 0 + threeD,
                                    data = .) ),
           coef_inf = map(model, tidy))
  
  #Get coefficients
  sample_coefs <- sample_models %>% 
    unnest(coef_inf)
  
  #Get confidence interval 
  percentile_intervals <- int_pctl(sample_models,
                                   coef_inf)

  #Store results
  results_boot_ft <- as.data.frame(cbind(mean(sample_coefs$estimate), (1/percentile_intervals$.estimate))) %>%
    rename(slope = V1, plot.slope= V2)

  slopes_boot_ft <- sample_coefs$estimate 
  
  
  print(results_boot_ft)


```

#manual p value
```{r}
pvalue <- sum(slopes_boot_ognp_v < slopes_boot_v)/1001 
```

#uncertainty
```{r}
# if (param == "ft") {
#   if (geo == "hex") {
#       slope <- hex_results_ft %>% select(slope) %>% as.numeric()
#       sample_df <- cbind(select(hex, sample, s.ft, db.ft, ri, size.cat, j.w1), slope)
#   }
#   if (geo == "ellip") {
#     slope <- ellip_results_ft %>% select(slope) %>% as.numeric()
#     sample_df <- cbind(select(ellip, sample, s.ft, db.ft, ri, size.cat, j.w1), slope)
#   }

slope <- mean(slopes_boot_ft)
sample_df <- cbind(select(hex, sample, s.ft, db.ft, size.cat), slope)  %>%
  filter(size.cat == "rare- small")

residual <- sample_df$s.ft - (sample_df$slope * sample_df$db.ft)
uncert.resid <- sd(residual)

qt(0.975, df = 15) * uncert.resid/sqrt(16) * 100


  percent_diff <- ((sample_df$s.ft - (sample_df$slope * sample_df$db.ft)) / sample_df$s.ft.ognp) * 100 
  uncert <- sd(percent_diff) #/ sqrt(length(filter(hex, size.cat == "common")))
  #percent.diff = (actual - yhat) / actual * 100
  #(actual - yhat = residual)

```




```{r}
apatite %>%
  filter(np.ognp == "term0") %>%
ggplot() + 
  geom_point(aes(x = s.ft, y = db.ft)) +  
  geom_point(aes(x = s.ft.ognp, y = db.ft), color =  "blue")

```






# Bootstrap p-value comparison code: 

Groupings of interest for comparison:
```{r}
grouping_gem <- c("A1", "A2", "A", "B1", "B2", "B", "C1", "C2", "C")
grouping_geo <- c("hexagonal", "ellipsoid")
# grouping_ri <- c("1", "2")
# grouping_term <- c("term0", "term1", "term2")
# grouping_size <- c("rare- small", "common", "rare- large")
# grouping_all <- c("A1", "A2", "A", "B1", "B2", "B", "C1", "C2", "C", "hexagonal", "ellipsoid", "1", "2", "term0", "term1", "term2", "rare- small", "common", "rare- large")
```

Calculate p-value to compare GEM against each other:
```{r}
################ 90% +/- a few percent...
params <- "esr_gem"
grouping <- grouping_gem

manual.pvalue(slopes_boot_esr, 
              glue("{grouping}"), 
              glue("{params}"), 
              0.90, 0.1) 

```

Uncertainty 
```{r}
# Ft
#hex_noterm <- no.term.ends %>% filter(geo == "hexagonal")
#residual.uncertainty(hex_noterm, "ft", "hexagonal")
#ggplotly(p, tooltip = c("key"))

residual.uncertainty(hex, "ft", "hexagonal")
residual.uncertainty(ellip, "ft", "ellipsoid")

# Volume
residual.uncertainty(hex, "volume", "hexagonal")
residual.uncertainty(ellip, "volume", "ellipsoid")


# ESR
residual.uncertainty(hex, "esr", "hexagonal")
residual.uncertainty(ellip, "esr", "ellipsoid")
```





################################################################################
Bootstrapping Code 

# Nested Dataframe REQUIRED for all linear regressions below: 
```{r}
# Created nested dataframe. Can add parameters here, just change the "11" in "nrow" when compiling results into a dataframe. 

sample_df <- apatite %>% 
  select(gem, gc, ri, geo, size.cat, db.ft, s.ft, db.v, s.v, s.esr.ft, db.esr.ft) %>% 
  pivot_longer(1:5, values_to = "grouping") %>% 
  group_by(name, grouping) %>% 
  nest()
```

# Nested Bootstrapped Linear Regression:
## Forcing the Intercept Through 0
This code chunk creates the results (ie. mean slope, mean std.err, plot slope) & creates vectors of bootstrapped slopes for all categories

```{r}
# Use 'map' to apply function over a list of dataframes (ie. the nested df above). 
param <-  "volume" 
#MUST CHANGE THIS FOR EACH PARAMETER YOU WANT TO RUN #ie. "ft", "volume", "esr"

results_boot <- pmap(list(sample_df[[3]], 
                           glue("{param}")), 
                     bootstrap.linreg.nest)

# Convert the lists of lists spit out by 'map' into a dataframe that is pretty & in the correct order. 
# Separate slopes from other results
results_flat <- flatten(results_boot) 
results_boot <- results_flat[seq(1, length(results_flat), 3)]
slopes_boot <- results_flat[seq(2, length(results_flat), 3)]
residuals_og <- results_flat[seq(3, length(results_flat), 3)]

# results --> tidy dataframe
results_boot_df <- data.frame(matrix(unlist(results_boot), nrow = 16, byrow = T))  %>% 
  rename(slope = X1, std.err = X2, plot.slope = X3) %>%
  bind_cols(sample_df[,2]) %>%
  relocate(grouping, .before = slope) %>%
  tibble() %>%
  mutate(grouping = factor(grouping, levels = c("A", "A1", "A2", "B", "B1", "B2", "C", "C1", "C2", "hexagonal", "ellipsoid", "1", "2", "rare- small", "common", "rare- large"))) %>%
  #mutate(grouping = factor(grouping, levels = c("A", "A1", "A2", "B", "B1", "B2", "hexagonal", "1", "2", "term0", "term1", "term2", "rare- small", "common", "rare- large"))) %>%
  arrange(grouping)

#Save results to named dataframe 
assign(paste("results_boot_noterm", glue("{param}"), sep =  "_"), results_boot_df)

# slopes --> tidy dataframe
# To calculate an uncertainty on the correction, a vector of all 1001 slopes split out by the bootstrapping procedure is saved into its own file.
slopes_boot_df <- data.frame(matrix(unlist(slopes_boot), nrow = 1001, byrow = FALSE)) %>%
  rename(B2 = X1, B = X2, `2` = X3, hexagonal = X4, `rare- small` = X5, A1 = X6, A = X7, `1` = X8, B1 = X9, A2 = X10, common = X11, C2 = X12, C = X13, ellipsoid = X14, C1 = X15, `rare- large` = X16) %>%
  tibble() %>%
  select("A", "A1", "A2", "B", "B1", "B2", "C", "C1", "C2", "hexagonal", "ellipsoid", "1", "2", "rare- small", "common", "rare- large")

#Save results to named dataframe 
assign(paste("slopes_boot_noterm", glue("{param}"), sep =  "_"), slopes_boot_df)
```

Save all the outputs you just made to excel files so you don't have to run this slow-ass code again. 
```{r}
filename <- "Regression Results_Final.xlsx"

###### Bootstrapped results,intercept fixed at zero
xlsx::write.xlsx(as.data.frame(results_boot_ft), file = glue("{filename}"), sheetName="results_boot_ft", append = TRUE, row.names=FALSE)

xlsx::write.xlsx(as.data.frame(results_boot_volume), file = glue("{filename}"), sheetName= "results_boot_volume", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(results_boot_esr), file=glue("{filename}"), sheetName="results_boot_esr", row.names=FALSE, append = TRUE)

###### Slopes, bootstrapped, intercept fixed at zero
xlsx::write.xlsx(as.data.frame(slopes_boot_ft), file=glue("{filename}"), sheetName="slopes_boot_ft", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(slopes_boot_volume), file=glue("{filename}"), sheetName="slopes_boot_volume", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(slopes_boot_esr), file=glue("{filename}"), sheetName="slopes_boot_esr", row.names=FALSE, append = TRUE)
```



######### INTERCEPT NOT FORCED THROUGH ZERO 

# Nested Bootstrapped Linear Regression:
## NOT Forcing the Intercept Through 0
This code chunk creates the results (ie. mean slope, mean std.err, plot slope) and creates vectors of bootstrapped intercepts for all categories
```{r}
param <- "volume" #MUST CHANGE THIS FOR EACH PARAMETER YOU WANT TO RUN #ie. "ft", "volume", "esr"

results_nf_boot <- pmap(list(sample_df[[3]], 
                          glue("{param}")), 
                     bootstrap.linreg.nest.not.fixed)

# Convert the lists of lists spit out by 'map' into a dataframe that is pretty & in the correct order. 
# Separate slopes from other results
results_nf_flat <- flatten(results_nf_boot) 
results_nf_boot <- results_nf_flat[seq(1, length(results_nf_flat), 3)]
intercepts_boot <- results_nf_flat[seq(2, length(results_nf_flat), 3)]
slopes_nf_boot <- results_nf_flat[seq(3, length(results_nf_flat), 3)]
  
results_nf_boot_df <- data.frame(matrix(unlist(results_nf_boot), nrow = 16, byrow = T)) %>%
  rename(slope = X1, intercept = X2, std.err = X3, plot.slope = X4, lower.conf.intercept = X5, upper.conf.intercept = X6) %>%
  bind_cols(sample_df[,2]) %>%
  relocate(grouping, .before = slope.mean) %>%
  tibble() %>%
  mutate(grouping = factor(grouping, levels = c("A", "A1", "A2", "B", "B1", "B2", "C", "C1", "C2", "hexagonal", "ellipsoid", "1", "2", "term0", "term1", "term2", "rare- small", "common", "rare- large"))) %>%
  arrange(grouping)

#Save results to named dataframe 
assign(paste("results_nf_boot", glue("{param}"), sep =  "_"), results_nf_boot_df)

# intercepts --> tidy dataframe
# To calculate an uncertainty on the correction, a vector of all 1001 intercepts split out by the bootstrapping procedure is saved into its own file.

intercepts_boot_df <- data.frame(matrix(unlist(intercepts_boot), nrow = 1001, byrow = FALSE)) %>%
  rename(C2 = X1, C = X2, `2` = X3, term1 = X4, ellipsoid = X5, common = X6, term0 = X7, C1 = X8, `1` = X9, term2 = X10, `rare- large` = X11, B2 = X12, B = X13, hexagonal = X14, B1 = X15, A2 = X16, A = X17, A1 = X18, `rare- small` = X19) %>%
  tibble() %>%
  select("A", "A1", "A2", "B", "B1", "B2", "C", "C1", "C2", "hexagonal", "ellipsoid", "1", "2", "term0", "term1", "term2", "rare- small", "common", "rare- large")

#Save results to named dataframe 
assign(paste("intercepts_boot", glue("{param}"), sep =  "_"), intercepts_boot_df)

# slopes --> tidy dataframe
# To calculate an uncertainty on the correction, a vector of all 1001 slopes split out by the bootstrapping procedure is saved into its own file.

slopes_nf_boot_df <- data.frame(matrix(unlist(slopes_nf_boot), nrow = 1001, byrow = FALSE)) %>%
  rename(C2 = X1, C = X2, `2` = X3, term1 = X4, ellipsoid = X5, common = X6, term0 = X7, C1 = X8, `1` = X9, term2 = X10, `rare- large` = X11, B2 = X12, B = X13, hexagonal = X14, B1 = X15, A2 = X16, A = X17, A1 = X18, `rare- small` = X19) %>%
  tibble() %>%
  select("A", "A1", "A2", "B", "B1", "B2", "C", "C1", "C2", "hexagonal", "ellipsoid", "1", "2", "term0", "term1", "term2", "rare- small", "common", "rare- large")

#Save results to named dataframe 
assign(paste("slopes_nf_boot", glue("{param}"), sep =  "_"), slopes_nf_boot_df)
```

Save all the outputs you just made to excel files so you don't have to run this slow-ass code again. 
```{r}
##### Bootstrapped results,intercept not fixed at zero

xlsx::write.xlsx(as.data.frame(results_nf_boot_ft), file="Regression Results_Final.xlsx", sheetName="results_nf_boot_ft", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(results_nf_boot_volume), file="Regression Results_Final.xlsx", sheetName="results_nf_boot_volume", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(results_nf_boot_esr), file="Regression Results_Final.xlsx", sheetName="results_nf_boot_esr", row.names=FALSE, append = TRUE)

###### Intercepts, bootstrapped, intercept not fixed at zero

xlsx::write.xlsx(as.data.frame(intercepts_boot_ft), file="Regression Results_Final.xlsx", sheetName="intercepts_boot_ft", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(intercepts_boot_volume), file="Regression Results_Final.xlsx", sheetName="intercepts_boot_volume", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(intercepts_boot_esr), file="Regression Results_Final.xlsx", sheetName="intercepts_boot_esr", row.names=FALSE, append = TRUE)

###### Slopes, bootstrapped, intercept not fixed at zero

xlsx::write.xlsx(as.data.frame(slopes_nf_boot_ft), file="Regression Results_Final.xlsx", sheetName="slopes_nf_boot_ft", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(slopes_nf_boot_volume), file="Regression Results_Final.xlsx", sheetName="slopes_nf_boot_volume", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(slopes_nf_boot_esr), file="Regression Results_Final.xlsx", sheetName="slopes_nf_boot_esr", row.names=FALSE, append = TRUE)
```


################################################################################
Taylor Code 

# Taylor Linear Regression:
## Forcing the Intercept Through 0
This code chunk creates the results (ie. slope, sigma slope, plot slope)
```{r}
param <- "esr"

results_taylor <- pmap(list(sample_df[[3]], 
                            glue("{param}")), 
                       taylor.uncertainty)

results_taylor_df <- data.frame(matrix(unlist(results_taylor), nrow = 19, byrow = TRUE)) %>%
  rename(slope = X1, sigma.slope = X2, plot.slope = X3) %>%
  bind_cols(sample_df[,2], sample_df[,1]) %>%
  relocate(grouping, name, .before = slope) %>%
  tibble() %>%
  mutate(grouping = factor(grouping, levels = c("A", "A1", "A2", "B", "B1", "B2", "C", "C1", "C2", "hexagonal", "ellipsoid", "1", "2", "term0", "term1", "term2", "rare- small", "common", "rare- large"))) %>%
  arrange(grouping)

#Save results to named dataframe 
assign(paste("results_taylor", glue("{param}"), sep =  "_"), results_taylor_df)
```

Save all the outputs you just made to excel files so you don't have to run this slow-ass code again. 
```{r}
filename <- "Regression Results_Final.xlsx"
#glue("{filename}")
###### Taylor results, intercept fixed at zero

xlsx::write.xlsx(as.data.frame(results_taylor_ft), file= glue("{filename}"), sheetName="results_taylor_ft", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(results_taylor_volume), file = glue("{filename}"), sheetName="results_taylor_volume", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(results_taylor_esr), file = glue("{filename}"), sheetName="results_taylor_esr", row.names=FALSE, append = TRUE)
```

# Taylor Linear Regression:
## NOT Forcing the Intercept Through 0
This code chunk creates the results (ie. slope, intercept, sigma slope, sigma intercept...)
```{r}
param <- "esr"
results_nf_taylor <- pmap(list(sample_df[[3]], 
                            glue("{param}")), 
                       taylor.uncertainty.not.fixed)

results_nf_taylor_df <- data.frame(matrix(unlist(results_nf_taylor), nrow = 19, byrow = TRUE)) %>%
  rename(slope = X1, sigma.slope = X2, intercept = X3, sigma.intercept =X4, plot.slope = X5) %>%
  bind_cols(sample_df[,2], sample_df[,1]) %>%
  relocate(grouping, name, .before = slope) %>%
  tibble() %>%
  mutate(grouping = factor(grouping, levels = c("A", "A1", "A2", "B", "B1", "B2", "C", "C1", "C2", "hexagonal", "ellipsoid", "1", "2", "term0", "term1", "term2", "rare- small", "common", "rare- large"))) %>%
  arrange(grouping)

#Save results to named dataframe 
assign(paste("results_nf_taylor", glue("{param}"), sep =  "_"), results_nf_taylor_df)
```

Save all the outputs you just made to excel files so you don't have to run this slow-ass code again. 
```{r}
filename <- "Regression Results_Final.xlsx"

###### Taylor results,intercept not fixed at zero

xlsx::write.xlsx(as.data.frame(results_nf_taylor_ft), file =  glue("{filename}"), sheetName="results_nf_taylor_ft", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(results_nf_taylor_volume), file =  glue("{filename}"), sheetName="results_nf_taylor_volume", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(results_nf_taylor_esr), file = glue("{filename}"), sheetName="results_nf_taylor_esr", row.names=FALSE, append = TRUE)
```

Eventually, the code to compare Taylor results easily to see if they overlap within 1sigma will exist: 

# Slope compare:
```{r}
params <- "volume_gem"
grouping <- grouping_gem

taylor.overlap(results_taylor_volume, 
               glue("{grouping}"), 
               glue("{params}"))
```

# Intercept compare:









