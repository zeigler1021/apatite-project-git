---
title: "Apatite Data_Stats"
author: "Spencer  Zeigler"
date: "6/15/2021"
  output:
  html_document: 
    df_print: paged
    css: stylesheet.css
    number_sections: yes
    toc: yes
    toc_float: true
    toc_depth: 3
    code_folding: show
editor_options:
  chunk_output_type: console
---

#Below is testing/working code: 

#Corrections
```{r}
#Ft
bootstrap.linreg(hex, "ft", "ft_hex")
bootstrap.linreg(ellip, "ft", "ft_ellip")

bootstrap.linreg(hex, "s.ft232", "db.ft232", "ft232_hex")
bootstrap.linreg(ellip, "s.ft232", "db.ft232", "ft232_ellip")

bootstrap.linreg(hex, "s.ft235", "db.ft235", "ft235_hex")
bootstrap.linreg(ellip, "s.ft235", "db.ft235", "ft235_ellip")

bootstrap.linreg(hex, "s.ft147", "db.ft147", "ft147_hex")
bootstrap.linreg(ellip, "s.ft147", "db.ft147", "ft147_ellip")

#Volume
bootstrap.linreg(hex, "volume", "vol_hex")
bootstrap.linreg(ellip, "volume", "vol_ellip")

#Rs
bootstrap.linreg(hex, "rs", "rs_hex")
bootstrap.linreg(ellip,"rs", "rs_ellip")

#Write vectors of corrections to csv. Import later if needed to do comparisons (pvalue tests)
correction_vectors <- as.data.frame(cbind(ft_hex_corr, ft_ellip_corr, ft232_hex_corr, ft232_ellip_corr, ft235_hex_corr, ft235_ellip_corr, ft147_hex_corr, ft147_ellip_corr, rs_hex_corr, rs_ellip_corr, vol_hex_corr, vol_ellip_corr))
write_csv(correction_vectors, "./outputs/correction_vectors_10182021")
```

#Uncertainties 
```{r}
#Ft Residuals
residual.sd(hex, ft_hex_results, "s.ft", "db.ft", "ft_hex")
residual.sd(ellip, ft_ellip_results, "s.ft", "db.ft", "ft_ellip")

residual.sd(hex, ft147_hex_results, "s.ft147", "db.ft147", "ft147_hex")
residual.sd(ellip, ft147_ellip_results, "s.ft147", "db.ft147", "ft147_ellip")

residual.sd(hex, ft232_hex_results, "s.ft232", "db.ft232", "ft232_hex")
residual.sd(ellip, ft232_ellip_results, "s.ft232", "db.ft232", "ft232_ellip")

residual.sd(hex, ft235_hex_results, "s.ft235", "db.ft235", "ft235_hex")
residual.sd(ellip, ft235_ellip_results, "s.ft235", "db.ft235", "ft235_ellip")

#Rs Residuals
residual.sd(hex, rs_hex_results, "s.rs", "db.rs", "rs_hex")
residual.sd(ellip, rs_ellip_results, "s.rs", "db.rs", "rs_ellip")

#Volume Residuals
residual.sd(hex, vol_hex_results, "s.v", "db.v", "vol_hex")
residual.sd(ellip, vol_ellip_results, "s.v", "db.v", "vol_ellip")
```

#Creating main results dataframe 
```{r}
#Combine residual df's by geometry first 
#Ft
ft_residuals <- full_join(ft_hex_residuals, ft_ellip_residuals) 
ft147_residuals <- full_join(ft147_hex_residuals, ft147_ellip_residuals) 
ft232_residuals <- full_join(ft232_hex_residuals, ft232_ellip_residuals) 
ft235_residuals <- full_join(ft235_hex_residuals, ft235_ellip_residuals) 
#Rs
rs_residuals <- full_join(rs_hex_residuals, rs_ellip_residuals) 
#Volume
vol_residuals <- full_join(vol_hex_residuals, vol_ellip_residuals) 

#Create "main" results dataframe with columns I care about
apatite_main <- select(apatite, sample, gem, gc, ri, np.obs, np.2, size.cat, geo, j.w1, db.ft, s.ft, db.v, s.v, db.rs, s.rs, db.ft232, s.ft232, db.ft235, s.ft235, db.ft147, s.ft147)

#Join residuals data (contains correction, percent diff, residual)
#Ft
apatite_main <- left_join(apatite_main, ft_residuals, by = "sample") %>%
  rename(corr.ft = corr, residual.ft = residual, percent_diff.ft = percent_diff)
apatite_main <- left_join(apatite_main, ft147_residuals, by = "sample") %>%
  rename(corr.ft147 = corr, residual.ft147 = residual, percent_diff.ft147 = percent_diff)
apatite_main <- left_join(apatite_main, ft232_residuals, by = "sample") %>%
  rename(corr.ft232 = corr, residual.ft232 = residual, percent_diff.ft232 = percent_diff)
apatite_main <- left_join(apatite_main, ft235_residuals, by = "sample") %>%
  rename(corr.ft235 = corr, residual.ft235 = residual, percent_diff.ft235 = percent_diff)
#Rs
apatite_main <- left_join(apatite_main, rs_residuals, by = "sample") %>%
  rename(corr.rs = corr, residual.rs = residual, percent_diff.rs = percent_diff)
#Volume
apatite_main <- left_join(apatite_main, vol_residuals, by = "sample") %>%
  rename(corr.v = corr, residual.v = residual, percent_diff.v = percent_diff)

#Add in the slopes for plotting!
apatite_main <- apatite_main %>%
  mutate(slope.ft = 1/apatite_main$corr.ft, 
         slope.ft147 = 1/apatite_main$corr.ft147, 
         slope.ft232 = 1/apatite_main$corr.ft232, 
         slope.ft235 = 1/apatite_main$corr.ft235, 
         slope.rs = 1/apatite_main$corr.rs, 
         slope.v = 1/apatite_main$corr.v)

#save to workbook for easy import 
write_csv(apatite_main, "./outputs/main_results_22102021.csv")
```

# Below outputs final results in a 'pretty' and organized way. Not good for testing and working. 

Notes: 
* All 'termination end' grains have been removed
* All terminations have been set to Np = 2 so that our 2D calculations assume ejection through those surfaces, just like they do for Blob3D 
* The intercept is being forced through 0 for these regressions 

Workflow: 
* Compute regressions for all parameters of interest (in this case, all GEM values)
* Double check slope uncertainties (by manually calculating a p-value) to confirm that A and B grains are hexagonal and  distinct from ellipsoid C grains within a ~90% confidence
* Determine value of the slope by bootstrapping the regression (2D ~ 0 + 3D)
* Determine the uncertainty on the correction by analyzing the residuals calculated from the mean of the bootstrapped slopes (ie. the correction). The residuals are calculated and then turned into a percent difference using the residual and the actual value. This normalizes the residuals. 
* At the end we have a correction (slope) Â± uncertainty (normalized residuals)

Wow! Years of work to end up with two frickin' numbers. God damn. 

Hard to believe. 

## Bootstrapping
 Goal: compute results for all GEM values
 # Nested Dataframe REQUIRED for all linear regressions below: 
```{r}
# Created nested dataframe.

sample_df <- apatite %>% 
  select(gem, gc, geo, db.ft, s.ft, db.v, s.v, s.esr.ft, db.esr.ft) %>% 
  pivot_longer(1:3, values_to = "grouping") %>% 
  group_by(name, grouping) %>% 
  nest()
```

# Nested Bootstrapped Linear Regression:
## Forcing the Intercept Through 0
This code chunk creates the results (ie. mean slope, mean std.err, plot slope) & creates vectors of bootstrapped slopes for all categories

```{r}
# Use 'map' to apply function over a list of dataframes (ie. the nested df above). 
param <-  "esr" 
#MUST CHANGE THIS FOR EACH PARAMETER YOU WANT TO RUN #ie. "ft", "volume", "esr"

results_boot <- pmap(list(sample_df[[3]], 
                           glue("{param}")), 
                     bootstrap.linreg.nest)

# Convert the lists of lists spit out by 'map' into a dataframe that is pretty & in the correct order. 
# Separate slopes from other results
results_flat <- flatten(results_boot) 
results_boot <- results_flat[seq(1, length(results_flat), 3)]
slopes_boot <- results_flat[seq(2, length(results_flat), 3)]
residuals_og <- results_flat[seq(3, length(results_flat), 3)]

# results --> tidy dataframe
results_boot_df <- data.frame(matrix(unlist(results_boot), nrow = 11, byrow = T))  %>% 
  rename(slope = X1, std.err = X2, plot.slope = X3) %>%
  bind_cols(sample_df[,2]) %>%
  relocate(grouping, .before = slope) %>%
  tibble() %>%
  mutate(grouping = factor(grouping, levels = c("A", "A1", "A2", "B", "B1", "B2", "C", "C1", "C2", "hexagonal", "ellipsoid"))) %>%
  arrange(grouping)

#Save results to named dataframe 
assign(paste("results_boot", glue("{param}"), sep =  "_"), results_boot_df)

# slopes --> tidy dataframe
# To calculate an uncertainty on the correction, a vector of all 1001 slopes split out by the bootstrapping procedure is saved into its own file.
slopes_boot_df <- data.frame(matrix(unlist(slopes_boot), nrow = 1001, byrow = FALSE)) %>%
  rename(B2 = X1, B = X2, hexagonal = X3, A1 = X4, A = X5, B1 = X6, A2 = X7, C2 = X8, C = X9, ellipsoid = X10, C1 = X11) %>%
  tibble() %>%
  select("A", "A1", "A2", "B", "B1", "B2", "C", "C1", "C2", "hexagonal", "ellipsoid")

#Save results to named dataframe 
assign(paste("slopes_boot", glue("{param}"), sep =  "_"), slopes_boot_df)
```

Save all the outputs you just made to excel files so you don't have to run this slow-ass code again. 
```{r}
filename <- "Regression Results_Final.xlsx"

###### Bootstrapped results,intercept fixed at zero
xlsx::write.xlsx(as.data.frame(results_boot_ft), file = glue("{filename}"), sheetName="results_boot_ft", append = TRUE, row.names=FALSE)

xlsx::write.xlsx(as.data.frame(results_boot_volume), file = glue("{filename}"), sheetName= "results_boot_volume", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(results_boot_esr), file=glue("{filename}"), sheetName="results_boot_esr", row.names=FALSE, append = TRUE)

###### Slopes, bootstrapped, intercept fixed at zero
xlsx::write.xlsx(as.data.frame(slopes_boot_ft), file=glue("{filename}"), sheetName="slopes_boot_ft", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(slopes_boot_volume), file=glue("{filename}"), sheetName="slopes_boot_volume", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(slopes_boot_esr), file=glue("{filename}"), sheetName="slopes_boot_esr", row.names=FALSE, append = TRUE)
```

Result: 
```{r bootstrap_gem_ft, fig.cap = "Table of regression slopes for each GEM value"}
#Ft
kable(results_boot_ft %>% 
  select(grouping, slope) %>% 
  pivot_longer(cols = grouping) %>% 
  select(-name) %>% 
  pivot_wider(names_from = value, values_from = slope))
```

```{r bootstrap_gem_ft, fig.cap = "Table of regression slopes for each GEM value"}
#Volume
kable(results_boot_volume %>% 
  select(grouping, slope) %>% 
  pivot_longer(cols = grouping) %>% 
  select(-name) %>% 
  pivot_wider(names_from = value, values_from = slope))
```

```{r bootstrap_gem_ft, fig.cap = "Table of regression slopes for each GEM value"}
#Rs
kable(results_boot_esr %>% 
  select(grouping, slope) %>% 
  pivot_longer(cols = grouping) %>% 
  select(-name) %>% 
  pivot_wider(names_from = value, values_from = slope))
```


## p-values 
Goal: compute p-values for all combinations of GEM values and determine which categories are significantly different or not. 

Groupings of interest for comparison:
```{r groupings}
grouping_gem <- c("A1", "A2", "A", "B1", "B2", "B", "C1", "C2", "C")
grouping_geo <- c("hexagonal", "ellipsoid")
```

```{r}
# Must run this code twice. Once with all the suffixes on grouping and params = gem and once with it = geo.

grouping <- grouping_gem
params_ft <- "ft_gem"
params_vol <- "volume_gem"
#params_esr <- "esr_geo"

manual.pvalue(slopes_boot_ft, 
              glue("{grouping}"), 
              glue("{params_ft}"), 
              0.90, 0.1) 


manual.pvalue(slopes_boot_volume, 
              glue("{grouping}"), 
              glue("{params_vol}"), 
              0.90, 0.1) 


# manual.pvalue(slopes_boot_esr, 
#               glue("{grouping}"), 
#               glue("{params_esr}"), 
#               0.90, 0.1) 


####To compare tet to normal 
sum(slope_boot_tet_ft$C > slope_boot_normal_ft$C)/1001

```

Result: A and B grains are not significantly different (p < 0.9). C grains are significantly different (p > 0.99)
```{r, echo=FALSE}
#Ft
kable(pvalue_ft_gem %>% 
        rename(`p-value` = pvalue) %>% 
        unite("GEM Compare", var1:var2, sep = " vs. ") %>% 
        relocate(`GEM Compare`, .before = `p-value`))


p.gem <- ggplot(pvalue_ft_gem, aes(x = var1, y = var2, fill = discrete)) + 
    geom_tile(color = "black") +
    theme_bw() +
    theme(axis.title = element_blank()) +
  theme(legend.position = "none") +
    geom_text(aes(label = round(pvalue,2)), size = 3, color = "black") +
    scale_fill_manual(values = tf_color,
                      breaks = c("Slopes are diff", "Slopes are same", "NA"),
                      labels = c("Slopes are different", "Slopes are same", "NA"), 
                      name = "Legend") 

p.geo <- ggplot(pvalue_ft_geo, aes(x = var1, y = var2, fill = discrete)) + 
    geom_tile(color = "black") +
    theme_bw() +
    theme(axis.title = element_blank()) +
  
    geom_text(aes(label = round(pvalue,2)), size = 3, color = "black") +
    scale_fill_manual(values = tf_color,
                      breaks = c("Slopes are diff", "Slopes are same", "NA"),
                      labels = c("Slopes are different", "Slopes are same", "NA"), 
                      name = "Legend") 


p.gem + p.geo + patchwork::plot_annotation(title = "Are the slopes of the regression for\neach GEM category significantly different or not? ", subtitle = "Ft Slopes at 90% Confidence")
```

```{r, echo = FALSE}
#Volume
kable(pvalue_volume_gem %>% 
        rename(`p-value` = pvalue) %>% 
        unite("GEM Compare", var1:var2, sep = " vs. ") %>% 
        relocate(`GEM Compare`, .before = `p-value`))

p.gem <- ggplot(pvalue_volume_gem, aes(x = var1, y = var2, fill = discrete)) + 
    geom_tile(color = "black") +
    theme_bw() +
    theme(axis.title = element_blank()) +
    theme(legend.position = "none") +
    geom_text(aes(label = round(pvalue,2)), size = 3, color = "black") +
    scale_fill_manual(values = tf_color,
                      breaks = c("Slopes are diff", "Slopes are same", "NA"),
                      labels = c("Slopes are different", "Slopes are same", "NA"), 
                      name = "Legend") 

p.geo <- ggplot(pvalue_volume_geo, aes(x = var1, y = var2, fill = discrete)) + 
    geom_tile(color = "black") +
    theme_bw() +
    theme(axis.title = element_blank()) +
    geom_text(aes(label = round(pvalue,2)), size = 3, color = "black") +
    scale_fill_manual(values = tf_color,
                      breaks = c("Slopes are diff", "Slopes are same", "NA"),
                      labels = c("Slopes are different", "Slopes are same", "NA"), 
                      name = "Legend") 
p.gem + p.geo + patchwork::plot_annotation(title = "Are the slopes of the regression for\neach GEM category significantly different or not? ", subtitle = "Volume Slopes at 90% Confidence")

```

```{r, echo = FALSE}
#Rs
kable(pvalue_esr_gem %>% 
        rename(`p-value` = pvalue) %>% 
        unite("GEM Compare", var1:var2, sep = " vs. ") %>% 
        relocate(`GEM Compare`, .before = `p-value`))

p.gem <- ggplot(pvalue_esr_gem, aes(x = var1, y = var2, fill = discrete)) + 
    geom_tile(color = "black") +
    theme_bw() +
    theme(axis.title = element_blank()) +
  theme(legend.position = "none") +
    geom_text(aes(label = round(pvalue,2)), size = 3, color = "black") +
    scale_fill_manual(values = tf_color,
                      breaks = c("Slopes are diff", "Slopes are same", "NA"),
                      labels = c("Slopes are different", "Slopes are same", "NA"), 
                      name = "Legend") 

p.geo <- ggplot(pvalue_esr_geo, aes(x = var1, y = var2, fill = discrete)) + 
    geom_tile(color = "black") +
    theme_bw() +
    theme(axis.title = element_blank()) +
  
    geom_text(aes(label = round(pvalue,2)), size = 3, color = "black") +
    scale_fill_manual(values = tf_color,
                      breaks = c("Slopes are diff", "Slopes are same", "NA"),
                      labels = c("Slopes are\ndifferent", "Slopes are same", "NA"), 
                      name = "Legend") 

p.gem + p.geo + patchwork::plot_annotation(title = "Are the slopes of the regression for\neach GEM category significantly different or not? ", subtitle = "Rs Slopes at 90% Confidence")
```


## Bootstrapping 
Goal: get the correction values we care about. We want a correction for all A & B grains (hexagonal) and a correction for all C grains (ellipsoid)
```{r}
#Must run this code 3 times. Once for "ft", "volume" "esr". Change it at "parameter". 
parameter <- "ft"

#Hexagonal Grains 
hex <- filter(apatite, gc == "A" | gc == "B")
bootstrap.linreg(hex, parameter, "hex")

#Ellipsoid Grains
ellip <- filter(apatite, gc == "C")
bootstrap.linreg(ellip, parameter, "ellip")

```

Result: 
```{r}
#Ft
kable(bind_rows(select(hex_results_ft, slope), select(ellip_results_ft,slope)) %>% bind_cols(c("Hexagonal", "Ellipsoid")) %>% rename(Geometry = ...2, Slope = slope) %>% relocate(Geometry), caption = "The correction (ie. slope of the regression) for each geometry subset for Ft. Hexagonal = A & B grains, Ellipsoid = C grains.")
#Volume
kable(bind_rows(select(hex_results_volume, slope), select(ellip_results_volume,slope)) %>% bind_cols(c("Hexagonal", "Ellipsoid")) %>% rename(Geometry = ...2, Slope = slope) %>% relocate(Geometry), caption = "The correction (ie. slope of the regression) for each geometry subset for Volume. Hexagonal = A & B grains, Ellipsoid = C grains.")
#Rs
kable(bind_rows(select(hex_results_esr, slope), select(ellip_results_esr,slope)) %>% bind_cols(c("Hexagonal", "Ellipsoid")) %>% rename(Geometry = ...2, Slope = slope) %>% relocate(Geometry), caption = "The correction (ie. slope of the regression) for each geometry subset for Rs. Hexagonal = A & B grains, Ellipsoid = C grains.")
```

## Uncertainties
Goal A: determine if there are parameters that control the uncertainty on the regression. We will analyze two parameters: roughness and size. Only for hexagonal grains. The C category is too small to split into further categories (n=37) so the uncertainty will be flat. 

```{r}
#Ft Hexagonal
residual.uncertainty("ft", "hex")
#Volume Hexagonal
residual.uncertainty("volume", "hex")
#Rs Hexagonal
#residual.uncertainty("esr", "hex")
```

Results:
```{r}
#Ft Hexagonal
plots_hex_ft[[1]] + plots_hex_ft[[2]] + plot_annotation(title = "Residuals for Hexagonal Grains for Ft", subtitle = "Is there a pattern within roughness or size?")
 
kable(table_hex_ft[[1]])
kable(table_hex_ft[[2]])

#Volume Hexagonal
plots_hex_volume[[1]] + plots_hex_volume[[2]] + plot_annotation(title = "Residuals for Hexagonal Grains for Volume", subtitle = "Is there a pattern within roughness or size?")
 
kable(table_hex_volume[[1]])
kable(table_hex_volume[[2]])

#ESR Hexagonal
# plots_hex_esr[[1]] + plots_hex_esr[[2]] + plot_annotation(title = "Residuals for Hexagonal Grains for Rs", subtitle = "Is there a pattern within roughness or size?")
#  
# kable(table_hex_esr[[1]])
# kable(table_hex_esr[[2]])
```
Our final results indicate that for Ft and Rs, there is a pattern with regard to size and for volume, there is a pattern with regards to roughness. Therefore, our final results reflect this. The uncertainties on the correction are controlled by roughness or size. 

Goal B: determine the uncertainty on the correction using the residuals from the linear regression and split by controlling parameter (ie. for volume, roughness. for Ft/Rs, size). 
```{r}
kable(table_hex_ft[[2]], caption = "Uncertainty on Ft correction split by size category for hexagonal grains.")
kable(table_hex_volume[[1]], caption = "Uncertainty on Volume correction split by grain roughness.")
kable(table_hex_esr[[2]], caption = "Uncertainty on Rs correction split by size category.")

#Ellipsoid results
residual.uncertainty("ft", "ellip")
residual.uncertainty("volume", "ellip")
residual.uncertainty("esr", "ellip")

kable(bind_cols(c(sd(uncert_result_ellip_ft$percent_diff), sd(uncert_result_ellip_volume$percent_diff), sd(uncert_result_ellip_esr$percent_diff)), c("Ft", "Volume", "Rs")) %>% rename(Uncertainty = ...1, Parameter = ...2) %>% relocate(Parameter, .before = Uncertainty), caption = "Uncertainty on corrections for ellipsoid grains. Due to small sample number (N=37), these uncertainties are not controlled by a parameter like roughness or size.")
```


Results: 
```{r}
#Print a table that shows the influence of secondary parameters
#Print a table that shows the final results 
```

## Results! 
Goal: we have corrected for the geometric uncertainty for Ft, volume (eU), and Rs! 
```{r}
#print corrections and results and celebrate good times come on
```




################ Post Thermo 2021 Testing

I want to test: 
Corrections for each isotope specific Ft (do they overlap with 238)?
- 232
- 235
- 147 

Bootstrapping: 
```{r}
sample_df <- hex %>% 
      #filter(np.ognp == "term1") %>%
      select(sample, gem, s.ft,  db.ft) %>% 
      rename(twoD = s.ft, threeD = db.ft) 


  set.seed(123)
  #Perform bootstrap 
  sample_boot <- bootstraps(sample_df,
                            times = 1000,
                            apparent = TRUE)
  
  #Run linear regression on each bootstrap
  sample_models <- sample_boot %>% 
    mutate(model = map(splits, ~ lm(twoD ~ 0 + threeD,
                                    data = .) ),
           coef_inf = map(model, tidy))
  
  #Get coefficients
  sample_coefs <- sample_models %>% 
    unnest(coef_inf)
  
  #Get confidence interval 
  percentile_intervals <- int_pctl(sample_models,
                                   coef_inf)

  #Store results
  results_boot_ft <- as.data.frame(cbind(mean(sample_coefs$estimate), (1/percentile_intervals$.estimate))) %>%
    rename(slope = V1, plot.slope= V2)

  slopes_boot_ft <- sample_coefs$estimate 
  
  
  print(results_boot_ft)


```

#manual p value
```{r}
pvalue <- sum(slopes_boot_ognp_v < slopes_boot_v)/1001 
```

#uncertainty
```{r}
# if (param == "ft") {
#   if (geo == "hex") {
#       slope <- hex_results_ft %>% select(slope) %>% as.numeric()
#       sample_df <- cbind(select(hex, sample, s.ft, db.ft, ri, size.cat, j.w1), slope)
#   }
#   if (geo == "ellip") {
#     slope <- ellip_results_ft %>% select(slope) %>% as.numeric()
#     sample_df <- cbind(select(ellip, sample, s.ft, db.ft, ri, size.cat, j.w1), slope)
#   }

slope <- mean(slopes_boot_ft)
sample_df <- cbind(select(hex, sample, s.ft, db.ft, size.cat), slope)  %>%
  filter(size.cat == "rare- small")

residual <- sample_df$s.ft - (sample_df$slope * sample_df$db.ft)
uncert.resid <- sd(residual)

qt(0.975, df = 15) * uncert.resid/sqrt(16) * 100


  percent_diff <- ((sample_df$s.ft - (sample_df$slope * sample_df$db.ft)) / sample_df$s.ft.ognp) * 100 
  uncert <- sd(percent_diff) #/ sqrt(length(filter(hex, size.cat == "common")))
  #percent.diff = (actual - yhat) / actual * 100
  #(actual - yhat = residual)

```




```{r}
apatite %>%
  filter(np.ognp == "term0") %>%
ggplot() + 
  geom_point(aes(x = s.ft, y = db.ft)) +  
  geom_point(aes(x = s.ft.ognp, y = db.ft), color =  "blue")

```






# Bootstrap p-value comparison code: 

Groupings of interest for comparison:
```{r}
grouping_gem <- c("A1", "A2", "A", "B1", "B2", "B", "C1", "C2", "C")
grouping_geo <- c("hexagonal", "ellipsoid")
# grouping_ri <- c("1", "2")
# grouping_term <- c("term0", "term1", "term2")
# grouping_size <- c("rare- small", "common", "rare- large")
# grouping_all <- c("A1", "A2", "A", "B1", "B2", "B", "C1", "C2", "C", "hexagonal", "ellipsoid", "1", "2", "term0", "term1", "term2", "rare- small", "common", "rare- large")
```

Calculate p-value to compare GEM against each other:
```{r}
################ 90% +/- a few percent...
params <- "esr_gem"
grouping <- grouping_gem

manual.pvalue(slopes_boot_esr, 
              glue("{grouping}"), 
              glue("{params}"), 
              0.90, 0.1) 

```

Uncertainty 
```{r}
# Ft
#hex_noterm <- no.term.ends %>% filter(geo == "hexagonal")
#residual.uncertainty(hex_noterm, "ft", "hexagonal")
#ggplotly(p, tooltip = c("key"))

residual.uncertainty(hex, "ft", "hexagonal")
residual.uncertainty(ellip, "ft", "ellipsoid")

# Volume
residual.uncertainty(hex, "volume", "hexagonal")
residual.uncertainty(ellip, "volume", "ellipsoid")


# ESR
residual.uncertainty(hex, "esr", "hexagonal")
residual.uncertainty(ellip, "esr", "ellipsoid")
```





################################################################################
Bootstrapping Code 

# Nested Dataframe REQUIRED for all linear regressions below: 
```{r}
# Created nested dataframe. Can add parameters here, just change the "11" in "nrow" when compiling results into a dataframe. 

sample_df <- apatite %>% 
  select(gem, gc, ri, geo, size.cat, db.ft, s.ft, db.v, s.v, s.esr.ft, db.esr.ft) %>% 
  pivot_longer(1:5, values_to = "grouping") %>% 
  group_by(name, grouping) %>% 
  nest()
```

# Nested Bootstrapped Linear Regression:
## Forcing the Intercept Through 0
This code chunk creates the results (ie. mean slope, mean std.err, plot slope) & creates vectors of bootstrapped slopes for all categories

```{r}
# Use 'map' to apply function over a list of dataframes (ie. the nested df above). 
param <-  "volume" 
#MUST CHANGE THIS FOR EACH PARAMETER YOU WANT TO RUN #ie. "ft", "volume", "esr"

results_boot <- pmap(list(sample_df[[3]], 
                           glue("{param}")), 
                     bootstrap.linreg.nest)

# Convert the lists of lists spit out by 'map' into a dataframe that is pretty & in the correct order. 
# Separate slopes from other results
results_flat <- flatten(results_boot) 
results_boot <- results_flat[seq(1, length(results_flat), 3)]
slopes_boot <- results_flat[seq(2, length(results_flat), 3)]
residuals_og <- results_flat[seq(3, length(results_flat), 3)]

# results --> tidy dataframe
results_boot_df <- data.frame(matrix(unlist(results_boot), nrow = 16, byrow = T))  %>% 
  rename(slope = X1, std.err = X2, plot.slope = X3) %>%
  bind_cols(sample_df[,2]) %>%
  relocate(grouping, .before = slope) %>%
  tibble() %>%
  mutate(grouping = factor(grouping, levels = c("A", "A1", "A2", "B", "B1", "B2", "C", "C1", "C2", "hexagonal", "ellipsoid", "1", "2", "rare- small", "common", "rare- large"))) %>%
  #mutate(grouping = factor(grouping, levels = c("A", "A1", "A2", "B", "B1", "B2", "hexagonal", "1", "2", "term0", "term1", "term2", "rare- small", "common", "rare- large"))) %>%
  arrange(grouping)

#Save results to named dataframe 
assign(paste("results_boot_noterm", glue("{param}"), sep =  "_"), results_boot_df)

# slopes --> tidy dataframe
# To calculate an uncertainty on the correction, a vector of all 1001 slopes split out by the bootstrapping procedure is saved into its own file.
slopes_boot_df <- data.frame(matrix(unlist(slopes_boot), nrow = 1001, byrow = FALSE)) %>%
  rename(B2 = X1, B = X2, `2` = X3, hexagonal = X4, `rare- small` = X5, A1 = X6, A = X7, `1` = X8, B1 = X9, A2 = X10, common = X11, C2 = X12, C = X13, ellipsoid = X14, C1 = X15, `rare- large` = X16) %>%
  tibble() %>%
  select("A", "A1", "A2", "B", "B1", "B2", "C", "C1", "C2", "hexagonal", "ellipsoid", "1", "2", "rare- small", "common", "rare- large")

#Save results to named dataframe 
assign(paste("slopes_boot_noterm", glue("{param}"), sep =  "_"), slopes_boot_df)
```

Save all the outputs you just made to excel files so you don't have to run this slow-ass code again. 
```{r}
filename <- "Regression Results_Final.xlsx"

###### Bootstrapped results,intercept fixed at zero
xlsx::write.xlsx(as.data.frame(results_boot_ft), file = glue("{filename}"), sheetName="results_boot_ft", append = TRUE, row.names=FALSE)

xlsx::write.xlsx(as.data.frame(results_boot_volume), file = glue("{filename}"), sheetName= "results_boot_volume", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(results_boot_esr), file=glue("{filename}"), sheetName="results_boot_esr", row.names=FALSE, append = TRUE)

###### Slopes, bootstrapped, intercept fixed at zero
xlsx::write.xlsx(as.data.frame(slopes_boot_ft), file=glue("{filename}"), sheetName="slopes_boot_ft", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(slopes_boot_volume), file=glue("{filename}"), sheetName="slopes_boot_volume", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(slopes_boot_esr), file=glue("{filename}"), sheetName="slopes_boot_esr", row.names=FALSE, append = TRUE)
```



######### INTERCEPT NOT FORCED THROUGH ZERO 

# Nested Bootstrapped Linear Regression:
## NOT Forcing the Intercept Through 0
This code chunk creates the results (ie. mean slope, mean std.err, plot slope) and creates vectors of bootstrapped intercepts for all categories
```{r}
param <- "volume" #MUST CHANGE THIS FOR EACH PARAMETER YOU WANT TO RUN #ie. "ft", "volume", "esr"

results_nf_boot <- pmap(list(sample_df[[3]], 
                          glue("{param}")), 
                     bootstrap.linreg.nest.not.fixed)

# Convert the lists of lists spit out by 'map' into a dataframe that is pretty & in the correct order. 
# Separate slopes from other results
results_nf_flat <- flatten(results_nf_boot) 
results_nf_boot <- results_nf_flat[seq(1, length(results_nf_flat), 3)]
intercepts_boot <- results_nf_flat[seq(2, length(results_nf_flat), 3)]
slopes_nf_boot <- results_nf_flat[seq(3, length(results_nf_flat), 3)]
  
results_nf_boot_df <- data.frame(matrix(unlist(results_nf_boot), nrow = 16, byrow = T)) %>%
  rename(slope = X1, intercept = X2, std.err = X3, plot.slope = X4, lower.conf.intercept = X5, upper.conf.intercept = X6) %>%
  bind_cols(sample_df[,2]) %>%
  relocate(grouping, .before = slope.mean) %>%
  tibble() %>%
  mutate(grouping = factor(grouping, levels = c("A", "A1", "A2", "B", "B1", "B2", "C", "C1", "C2", "hexagonal", "ellipsoid", "1", "2", "term0", "term1", "term2", "rare- small", "common", "rare- large"))) %>%
  arrange(grouping)

#Save results to named dataframe 
assign(paste("results_nf_boot", glue("{param}"), sep =  "_"), results_nf_boot_df)

# intercepts --> tidy dataframe
# To calculate an uncertainty on the correction, a vector of all 1001 intercepts split out by the bootstrapping procedure is saved into its own file.

intercepts_boot_df <- data.frame(matrix(unlist(intercepts_boot), nrow = 1001, byrow = FALSE)) %>%
  rename(C2 = X1, C = X2, `2` = X3, term1 = X4, ellipsoid = X5, common = X6, term0 = X7, C1 = X8, `1` = X9, term2 = X10, `rare- large` = X11, B2 = X12, B = X13, hexagonal = X14, B1 = X15, A2 = X16, A = X17, A1 = X18, `rare- small` = X19) %>%
  tibble() %>%
  select("A", "A1", "A2", "B", "B1", "B2", "C", "C1", "C2", "hexagonal", "ellipsoid", "1", "2", "term0", "term1", "term2", "rare- small", "common", "rare- large")

#Save results to named dataframe 
assign(paste("intercepts_boot", glue("{param}"), sep =  "_"), intercepts_boot_df)

# slopes --> tidy dataframe
# To calculate an uncertainty on the correction, a vector of all 1001 slopes split out by the bootstrapping procedure is saved into its own file.

slopes_nf_boot_df <- data.frame(matrix(unlist(slopes_nf_boot), nrow = 1001, byrow = FALSE)) %>%
  rename(C2 = X1, C = X2, `2` = X3, term1 = X4, ellipsoid = X5, common = X6, term0 = X7, C1 = X8, `1` = X9, term2 = X10, `rare- large` = X11, B2 = X12, B = X13, hexagonal = X14, B1 = X15, A2 = X16, A = X17, A1 = X18, `rare- small` = X19) %>%
  tibble() %>%
  select("A", "A1", "A2", "B", "B1", "B2", "C", "C1", "C2", "hexagonal", "ellipsoid", "1", "2", "term0", "term1", "term2", "rare- small", "common", "rare- large")

#Save results to named dataframe 
assign(paste("slopes_nf_boot", glue("{param}"), sep =  "_"), slopes_nf_boot_df)
```

Save all the outputs you just made to excel files so you don't have to run this slow-ass code again. 
```{r}
##### Bootstrapped results,intercept not fixed at zero

xlsx::write.xlsx(as.data.frame(results_nf_boot_ft), file="Regression Results_Final.xlsx", sheetName="results_nf_boot_ft", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(results_nf_boot_volume), file="Regression Results_Final.xlsx", sheetName="results_nf_boot_volume", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(results_nf_boot_esr), file="Regression Results_Final.xlsx", sheetName="results_nf_boot_esr", row.names=FALSE, append = TRUE)

###### Intercepts, bootstrapped, intercept not fixed at zero

xlsx::write.xlsx(as.data.frame(intercepts_boot_ft), file="Regression Results_Final.xlsx", sheetName="intercepts_boot_ft", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(intercepts_boot_volume), file="Regression Results_Final.xlsx", sheetName="intercepts_boot_volume", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(intercepts_boot_esr), file="Regression Results_Final.xlsx", sheetName="intercepts_boot_esr", row.names=FALSE, append = TRUE)

###### Slopes, bootstrapped, intercept not fixed at zero

xlsx::write.xlsx(as.data.frame(slopes_nf_boot_ft), file="Regression Results_Final.xlsx", sheetName="slopes_nf_boot_ft", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(slopes_nf_boot_volume), file="Regression Results_Final.xlsx", sheetName="slopes_nf_boot_volume", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(slopes_nf_boot_esr), file="Regression Results_Final.xlsx", sheetName="slopes_nf_boot_esr", row.names=FALSE, append = TRUE)
```


################################################################################
Taylor Code 

# Taylor Linear Regression:
## Forcing the Intercept Through 0
This code chunk creates the results (ie. slope, sigma slope, plot slope)
```{r}
param <- "esr"

results_taylor <- pmap(list(sample_df[[3]], 
                            glue("{param}")), 
                       taylor.uncertainty)

results_taylor_df <- data.frame(matrix(unlist(results_taylor), nrow = 19, byrow = TRUE)) %>%
  rename(slope = X1, sigma.slope = X2, plot.slope = X3) %>%
  bind_cols(sample_df[,2], sample_df[,1]) %>%
  relocate(grouping, name, .before = slope) %>%
  tibble() %>%
  mutate(grouping = factor(grouping, levels = c("A", "A1", "A2", "B", "B1", "B2", "C", "C1", "C2", "hexagonal", "ellipsoid", "1", "2", "term0", "term1", "term2", "rare- small", "common", "rare- large"))) %>%
  arrange(grouping)

#Save results to named dataframe 
assign(paste("results_taylor", glue("{param}"), sep =  "_"), results_taylor_df)
```

Save all the outputs you just made to excel files so you don't have to run this slow-ass code again. 
```{r}
filename <- "Regression Results_Final.xlsx"
#glue("{filename}")
###### Taylor results, intercept fixed at zero

xlsx::write.xlsx(as.data.frame(results_taylor_ft), file= glue("{filename}"), sheetName="results_taylor_ft", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(results_taylor_volume), file = glue("{filename}"), sheetName="results_taylor_volume", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(results_taylor_esr), file = glue("{filename}"), sheetName="results_taylor_esr", row.names=FALSE, append = TRUE)
```

# Taylor Linear Regression:
## NOT Forcing the Intercept Through 0
This code chunk creates the results (ie. slope, intercept, sigma slope, sigma intercept...)
```{r}
param <- "esr"
results_nf_taylor <- pmap(list(sample_df[[3]], 
                            glue("{param}")), 
                       taylor.uncertainty.not.fixed)

results_nf_taylor_df <- data.frame(matrix(unlist(results_nf_taylor), nrow = 19, byrow = TRUE)) %>%
  rename(slope = X1, sigma.slope = X2, intercept = X3, sigma.intercept =X4, plot.slope = X5) %>%
  bind_cols(sample_df[,2], sample_df[,1]) %>%
  relocate(grouping, name, .before = slope) %>%
  tibble() %>%
  mutate(grouping = factor(grouping, levels = c("A", "A1", "A2", "B", "B1", "B2", "C", "C1", "C2", "hexagonal", "ellipsoid", "1", "2", "term0", "term1", "term2", "rare- small", "common", "rare- large"))) %>%
  arrange(grouping)

#Save results to named dataframe 
assign(paste("results_nf_taylor", glue("{param}"), sep =  "_"), results_nf_taylor_df)
```

Save all the outputs you just made to excel files so you don't have to run this slow-ass code again. 
```{r}
filename <- "Regression Results_Final.xlsx"

###### Taylor results,intercept not fixed at zero

xlsx::write.xlsx(as.data.frame(results_nf_taylor_ft), file =  glue("{filename}"), sheetName="results_nf_taylor_ft", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(results_nf_taylor_volume), file =  glue("{filename}"), sheetName="results_nf_taylor_volume", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(results_nf_taylor_esr), file = glue("{filename}"), sheetName="results_nf_taylor_esr", row.names=FALSE, append = TRUE)
```

Eventually, the code to compare Taylor results easily to see if they overlap within 1sigma will exist: 

# Slope compare:
```{r}
params <- "volume_gem"
grouping <- grouping_gem

taylor.overlap(results_taylor_volume, 
               glue("{grouping}"), 
               glue("{params}"))
```

# Intercept compare:









