---
title: "Apatite Data_Stats_v5"
author: "Spencer  Zeigler"
date: "6/15/2021"
output: html_document
editor_options:
  chunk_output_type: console
---

Notes for Zach on uncertainty shit:
```{r}
pvalue <- sum(b_slopes_volume >= c_slopes_volume)/1000
sum(pvalue)
#The issue with this is that direction matter. (ie. if I put c first or a first). Also, I'm not sure what it means to get a p value of '0'-- just means significently different? But then if I switch the order I get a p value grater than 1. What?
abs(a_slopes_ft - b_slopes_ft)
abs(a_slopes_ft - c_slopes_ft)

#make 0 my test stat. subtract slopes and see if they are within 0 or not. try a confidence interval, maybe? (does the CI include 0 or not?). or just use a logical vector and count number of T/F and get a p value. check if answers are the same if you take the abs of the difference because otherwise the order depends. Or ask Zach? idk bitch. but i think zero should be the test stat. 

```

# Nested Dataframe REQUIRED for all linear regressions below: 
```{r}
# Created nested dataframe. Can add parameters here, just change the "11" in "nrow" when compiling results into a dataframe. 
sample_df <- apatite %>%
  select(gem, gc, ri, np, db.ft, s.ft, db.v, s.v, s.esr.ft, db.esr.ft) %>%
  pivot_longer(1:4, values_to = "grouping")%>%
  group_by(name, grouping) %>%
  nest()
```

# Nested Bootstrapped Linear Regression:
## Forcing the Intercept Through 0
This code chunk creates the results (ie. mean slope, mean std.err, plot slope) & creates vectors of bootstrapped slopes for all categories

```{r}
# Use 'map' to apply function over a list of dataframes (ie. the nested df above). 
param <-  "esr" 
#MUST CHANGE THIS FOR EACH PARAMETER YOU WANT TO RUN #ie. "ft", "volume", "esr"

results_boot <- pmap(list(sample_df[[3]], 
                           glue("{param}")), 
                     bootstrap.linreg.nest)

# Convert the lists of lists spit out by 'map' into a dataframe that is pretty & in the correct order. 
# Seperate slopes from other results
results_flat <- flatten(results_boot) 
results_boot <- results_flat[seq(1, length(results_flat), 2)]
slopes_boot <- results_flat[seq(2, length(results_flat), 2)]

# results --> tidy dataframe
results_boot_df <- data.frame(matrix(unlist(results_boot), nrow = 11, byrow = T)) %>%
  rename(slope = X1, std.err = X2, plot.slope = X3) %>%
  bind_cols(sample_df[,2]) %>%
  relocate(grouping, .before = slope) %>%
  tibble() %>%
  mutate(grouping = factor(grouping, levels = c("A", "A1", "A2", "B", "B1", "B2", "1", "2", "term0", "term1", "term2"))) %>%
  arrange(grouping)

#Save results to named dataframe 
assign(paste("results_boot", glue("{param}"), sep =  "_"), results_boot_df)

# slopes --> tidy dataframe
# To calculate an uncertainty on the correction, a vector of all 1001 slopes split out by the bootstrapping procedure is saved into its own file.
slopes_boot_df <- data.frame(matrix(unlist(slopes_boot), nrow = 1001, byrow = FALSE)) %>%
  rename(A1 = X1, A = X2, `1` = X3, term1 = X4, term2 = X5, A2 = X6, `2`= X7, term0 = X8, B2 = X9, B = X10, B1 = X11) %>%
  tibble() %>%
  select("A", "A1", "A2", "B", "B1", "B2", "1", "2", "term0", "term1", "term2")

#Save results to named dataframe 
assign(paste("slopes_boot", glue("{param}"), sep =  "_"), slopes_boot_df)
```

Save all the outputs you just made to excel files so you don't have to run this slow-ass code again. 
```{r}
filename <- "Regression Results_Final.xlsx"

###### Bootstrapped results,intercept fixed at zero
xlsx::write.xlsx(as.data.frame(results_boot_ft), file = glue("{filename}"), sheetName="results_boot_ft", append = TRUE, row.names=FALSE)

xlsx::write.xlsx(as.data.frame(results_boot_volume), file = glue("{filename}"), sheetName="results_boot_volume", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(results_boot_esr), file=glue("{filename}"), sheetName="results_boot_esr", row.names=FALSE, append = TRUE)

###### Slopes, bootstrapped, intercept fixed at zero
xlsx::write.xlsx(as.data.frame(slopes_boot_ft), file=glue("{filename}"), sheetName="slopes_boot_ft", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(slopes_boot_volume), file=glue("{filename}"), sheetName="slopes_boot_volume", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(slopes_boot_esr), file=glue("{filename}"), sheetName="slopes_boot_esr", row.names=FALSE, append = TRUE)
```

# Nested Bootstrapped Linear Regression:
## NOT Forcing the Intercept Through 0
This code chunk creates the results (ie. mean slope, mean std.err, plot slope) and creates vectors of bootstrapped intercepts for all categories
```{r}
param <- "esr" #MUST CHANGE THIS FOR EACH PARAMETER YOU WANT TO RUN #ie. "ft", "volume", "esr"

results_nf_boot <- pmap(list(sample_df[[3]], 
                          glue("{param}")), 
                     bootstrap.linreg.nest.not.fixed)

# Convert the lists of lists spit out by 'map' into a dataframe that is pretty & in the correct order. 
# Seperate slopes from other results
results_nf_flat <- flatten(results_nf_boot) 
results_nf_boot <- results_nf_flat[seq(1, length(results_nf_flat), 3)]
intercepts_boot <- results_nf_flat[seq(2, length(results_nf_flat), 3)]
slopes_nf_boot <- results_nf_flat[seq(3, length(results_nf_flat), 3)]
  
results_nf_boot_df <- data.frame(matrix(unlist(results_nf_boot), nrow = 11, byrow = T)) %>%
  rename(slope = X1, intercept = X2, std.err = X3, plot.slope = X4) %>%
  bind_cols(sample_df[,2]) %>%
  relocate(grouping, .before = slope) %>%
  tibble() %>%
  mutate(grouping = factor(grouping, levels = c("A", "A1", "A2", "B", "B1", "B2", "1", "2", "term0", "term1", "term2"))) %>%
  arrange(grouping)

#Save results to named dataframe 
assign(paste("results_nf_boot", glue("{param}"), sep =  "_"), results_nf_boot_df)

# intercepts --> tidy dataframe
# To calculate an uncertainty on the correction, a vector of all 1001 intercepts split out by the bootstrapping procedure is saved into its own file.

intercepts_boot_df <- data.frame(matrix(unlist(intercepts_boot), nrow = 1001, byrow = FALSE)) %>%
  rename(A1 = X1, A = X2, `1` = X3, term1 = X4, term2 = X5, A2 = X6, `2`= X7, term0 = X8, B2 = X9, B = X10, B1 = X11) %>%
  tibble() %>%
  select("A", "A1", "A2", "B", "B1", "B2", "1", "2", "term0", "term1", "term2")

#Save results to named dataframe 
assign(paste("intercepts_boot", glue("{param}"), sep =  "_"), intercepts_boot_df)

# slopes --> tidy dataframe
# To calculate an uncertainty on the correction, a vector of all 1001 slopes split out by the bootstrapping procedure is saved into its own file.

slopes_nf_boot_df <- data.frame(matrix(unlist(slopes_nf_boot), nrow = 1001, byrow = FALSE)) %>%
  rename(A1 = X1, A = X2, `1` = X3, term1 = X4, term2 = X5, A2 = X6, `2`= X7, term0 = X8, B2 = X9, B = X10, B1 = X11) %>%
  tibble() %>%
  select("A", "A1", "A2", "B", "B1", "B2", "1", "2", "term0", "term1", "term2")

#Save results to named dataframe 
assign(paste("slopes_nf_boot", glue("{param}"), sep =  "_"), slopes_nf_boot_df)
```

Save all the outputs you just made to excel files so you don't have to run this slow-ass code again. 
```{r}
##### Bootstrapped results,intercept not fixed at zero

xlsx::write.xlsx(as.data.frame(results_nf_boot_ft), file="Regression Results_Final.xlsx", sheetName="results_nf_boot_ft", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(results_nf_boot_volume), file="Regression Results_Final.xlsx", sheetName="results_nf_boot_volume", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(results_nf_boot_esr), file="Regression Results_Final.xlsx", sheetName="results_nf_boot_esr", row.names=FALSE, append = TRUE)

###### Intercepts, bootstrapped, intercept not fixed at zero

xlsx::write.xlsx(as.data.frame(intercepts_boot_ft), file="Regression Results_Final.xlsx", sheetName="intercepts_boot_ft", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(intercepts_boot_volume), file="Regression Results_Final.xlsx", sheetName="intercepts_boot_volume", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(intercepts_boot_esr), file="Regression Results_Final.xlsx", sheetName="intercepts_boot_esr", row.names=FALSE, append = TRUE)

###### Slopes, bootstrapped, intercept not fixed at zero

xlsx::write.xlsx(as.data.frame(slopes_nf_boot_ft), file="Regression Results_Final.xlsx", sheetName="slopes_nf_boot_ft", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(slopes_nf_boot_volume), file="Regression Results_Final.xlsx", sheetName="slopes_nf_boot_volume", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(slopes_nf_boot_esr), file="Regression Results_Final.xlsx", sheetName="slopes_nf_boot_esr", row.names=FALSE, append = TRUE)
```


# Taylor Linear Regression:
## Forcing the Intercept Through 0
This code chunk creates the results (ie. slope, sigma slope, plot slope)
```{r}
param <- "esr"

results_taylor <- pmap(list(sample_df[[3]], 
                            glue("{param}")), 
                       taylor.uncertainty)

results_taylor_df <- data.frame(matrix(unlist(results_taylor), nrow = 11, byrow = TRUE)) %>%
  rename(slope = X1, sigma.slope = X2, plot.slope = X3) %>%
  bind_cols(sample_df[,2], sample_df[,1]) %>%
  relocate(grouping, name, .before = slope) %>%
  tibble() %>%
  mutate(grouping = factor(grouping, levels = c("A", "A1", "A2", "B", "B1", "B2", "1", "2", "term0", "term1", "term2"))) %>%
  arrange(grouping)

#Save results to named dataframe 
assign(paste("results_taylor", glue("{param}"), sep =  "_"), results_taylor_df)
```

Save all the outputs you just made to excel files so you don't have to run this slow-ass code again. 
```{r}
filename <- "Regression Results_Final.xlsx"
#glue("{filename}")
###### Taylor results, intercept fixed at zero

xlsx::write.xlsx(as.data.frame(results_taylor_ft), file= glue("{filename}"), sheetName="results_taylor_ft", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(results_taylor_volume), file = glue("{filename}"), sheetName="results_taylor_volume", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(results_taylor_esr), file = glue("{filename}"), sheetName="results_taylor_esr", row.names=FALSE, append = TRUE)
```

# Taylor Linear Regression:
## NOT Forcing the Intercept Through 0
This code chunk creates the results (ie. slope, intercept, sigma slope, sigma intercept...)
```{r}
param <- "esr"
results_nf_taylor <- pmap(list(sample_df[[3]], 
                            glue("{param}")), 
                       taylor.uncertainty.not.fixed)

results_nf_taylor_df <- data.frame(matrix(unlist(results_nf_taylor), nrow = 11, byrow = TRUE)) %>%
  rename(slope = X1, sigma.slope = X2, intercept = X3, sigma.intercept =X4, plot.slope = X5) %>%
  bind_cols(sample_df[,2], sample_df[,1]) %>%
  relocate(grouping, name, .before = slope) %>%
  tibble() %>%
  mutate(grouping = factor(grouping, levels = c("A", "A1", "A2", "B", "B1", "B2", "1", "2", "term0", "term1", "term2"))) %>%
  arrange(grouping)

#Save results to named dataframe 
assign(paste("results_nf_taylor", glue("{param}"), sep =  "_"), results_nf_taylor_df)
```

Save all the outputs you just made to excel files so you don't have to run this slow-ass code again. 
```{r}
filename <- "Regression Results_Final.xlsx"
#glue("{filename}")
###### Taylor results,intercept not fixed at zero

xlsx::write.xlsx(as.data.frame(results_nf_taylor_ft), file =  glue("{filename}"), sheetName="results_nf_taylor_ft", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(results_nf_taylor_volume), file =  glue("{filename}"), sheetName="results_nf_taylor_volume", row.names=FALSE, append = TRUE)

xlsx::write.xlsx(as.data.frame(results_nf_taylor_esr), file = glue("{filename}"), sheetName="results_nf_taylor_esr", row.names=FALSE, append = TRUE)
```






Dead to me round 2:
################################################################################################


#Uncertainty Calculations
```{r}
apatite <- apatite %>%
  mutate(ri.cat = ifelse(ri == "3", "3", "12"))

df <- apatite %>%
  select(sample, gem.geo, ri, ri.cat, size.cat, s.ft, db.ft, s.v, db.v) %>%
  filter(gem.geo == "C") %>%
  nest_by(size.cat, ri.cat) 

models <- df %>%
  mutate(model = list(lm(s.v ~ 0 + db.v, data = data)))

#create db which are lists of 3d values the same length of each of the model results (residuals): this is for calculating the percent difference 
db <- vector("list", nrow(df))

for (i in 1:nrow(df)) {
  db[[i]] <- unlist(df[[3]][[i]]$db.v)
}

#extract each residual result
sample_residuals <- vector("list", nrow(df))
for (i in 1:nrow(df)) {
  sample_residuals[[i]] <- unlist(models$model[[i]][[2]])
}

#get final results 
results <-  matrix(nrow = nrow(df), ncol = 2)
median <- vector("numeric", nrow(df))
std.dev <- vector("numeric", nrow(df))
n <- vector("numeric", nrow(df))

for (i in 1:nrow(df)) {
  x <- sample_residuals[[i]]
  y <- db[[i]]
  
  percent.diff  <-(x/y)*100
  median[i] <- median(percent.diff)
  std.dev[i] <-  sd(percent.diff)
  n[i] <- length(x)
  
  results <- cbind(median, std.dev, n)
}
```

Creating uncertainty dataframes: 
### Only bit that needs to change is x, z, y. Need to change this before plotting.
```{r}
x <- c_all_residuals_ft[,1001] #extract residuals calculated from observed data
z <- c_all$db.ft #extract 'real' value to calculate % difference
y <- c_all #store dataframe that corresponds to calculated residuals

#Combine all columns from above & parameters we care about (ie. size bin and name)
original_residuals <- bind_cols(og.residuals = x,
                                percent.diff = (x/z)*100,
                                size.name = y$size.name, 
                                #size.bin = y$size.bin, 
                                size.cat = y$size.cat,
                                ri = y$ri) 
#Store just the percent difference value for plotting later
percent.diff <- original_residuals$percent.diff

#Format original residuals dataframe 
original_residuals <- pivot_longer(original_residuals, cols = 3:5, names_to = "category") 
#Set as factors to force output tables into the right order 
original_residuals$value <- factor(original_residuals$value, levels = c("rare- small", "common", "rare- large", "Small & Rare", "Small & Common", "Typical & Common", "Large & Common", "Large & Rare", "1", "2", "3"))

all_res <- original_residuals %>% 
  mutate(sign = "total") %>%
  group_by(value, sign) %>%
  summarise(median_res = median(percent.diff), sd_res = sd(percent.diff))

sign_res <- original_residuals %>%
  mutate(sign = ifelse(percent.diff > 0, "positive", "negative")) %>%
  group_by(value, sign) %>% 
  summarise(median_res = median(percent.diff), sd_res = sd(percent.diff))

res_plotting <- bind_rows(all_res, sign_res) 
res_results <- res_plotting %>%
  pivot_wider(values_from = 3:4, names_from = sign)
```


# t-test to assess if slopes are statistically the same or not
```{r}
ttest_compare(b_all_slopes_volume, c_all_slopes_volume)

t.test(b_slopes_volume, a_slopes_volume, conf.level = 0.6827)
x <- sample(ab_slopes_volume, 100)
y <- sample(a_slopes_volume, 100)
t.test(x,y)
#if t-test p value is < 2.2e-16, result will be a p-value of 0
#https://github.com/tidymodels/broom/issues/227

```


```{r}
linreg <- lm(s.ft ~ 0 + db.ft, gca)

std.err <- sqrt(sum((gca$s.ft - linreg$fitted.values)^2) / (nrow(gca) - 2))
sb <- std.err / sqrt(sum((gca$db.ft - mean(gca$db.ft))^2))

#This gives me the same uncertainty as the excel LINEST and for Taylor 
#https://sites.chem.utoronto.ca/chemistry/coursenotes/analsci/stats/ErrRegr.html
```


```{r}
results.a <- slope.uncertainty(gca, "ft")
results.b <- slope.uncertainty(gcb, "ft")
results.c <- slope.uncertainty(gcc, "ft")

t.test(results.a$slope, results.b$slope)

# m1, m2: the sample means
# s1, s2: the sample standard deviations
# n1, n2: the same sizes
# m0: the null value for the difference in means to be tested for. Default is 0. 
# equal.variance: whether or not to assume equal variance. Default is FALSE. 
t.test2 <- function(m1,m2,s1,s2,n1,n2,m0=0,equal.variance=FALSE)
{
    if( equal.variance==FALSE ) 
    {
        se <- sqrt( (s1^2/n1) + (s2^2/n2) )
        # welch-satterthwaite df
        df <- ( (s1^2/n1 + s2^2/n2)^2 )/( (s1^2/n1)^2/(n1-1) + (s2^2/n2)^2/(n2-1) )
    } else
    {
        # pooled standard deviation, scaled by the sample sizes
        se <- sqrt( (1/n1 + 1/n2) * ((n1-1)*s1^2 + (n2-1)*s2^2)/(n1+n2-2) ) 
        df <- n1+n2-2
    }      
    t <- (m1-m2-m0)/se 
    dat <- c(m1-m2, se, t, 2*pt(-abs(t),df))    
    names(dat) <- c("Difference of means", "Std Error", "t", "p-value")
    return(dat) 
}

t.test2(results.a$slope, results.b$slope, results.a$sigma.slope, results.b$sigma.slope, length(gca$s.ft), length(gcb$s.ft))

t.test2(mean(a_slopes_ft), mean(b_slopes_ft), sd(a_slopes_ft), sd(b_slopes_ft), 1001, 1001)
#A and B are equal

t.test2(results.a$slope, results.c$slope, results.a$sigma.slope, results.c$sigma.slope, length(gca$s.ft), length(gcc$s.ft))

#A and C are different (reject the null)

t.test2(results.b$slope, results.c$slope, results.b$sigma.slope, results.c$sigma.slope, length(gcb$s.ft), length(gcc$s.ft))

#B and C are different
```



Using bootstrap code to get vectors of slopes to perform t-tests on:
```{r}
    sample_df <- gcc %>% select(sample, s.ft,  db.ft) %>% rename(twoD = s.ft,  threeD = db.ft)
  
  # Set set.seed a starting number to generate a sequence of random numbers so that we can get reproducible results
  set.seed(123)
  #Perform bootstrap 
  sample_boot <- bootstraps(sample_df,
                            times = 1000,
                            apparent = TRUE)
  
  #Run linear regression on each bootstrap
  sample_models <- sample_boot %>% 
    mutate(model = map(splits, ~ lm(twoD ~ 0 + threeD,
                                    data = .) ),
           coef_inf = map(model, tidy))
  #Get coefficients
  sample_coefs <- sample_models %>% 
    unnest(coef_inf)
  

  
  #Get confidence interval 
  percentile_intervals <- int_pctl(sample_models,
                                   coef_inf)
  
  #Store results
  results <<- as.data.frame(cbind(percentile_intervals$.estimate, sd(sample_coefs$estimate),mean(sample_coefs$std.error), (1/percentile_intervals$.estimate))) %>%
    rename(slope = V1, sd = V2, std.error = V3, plot.slope= V4)
  
```

```{r}
lower.ci <- percentile_intervals$.estimate - percentile_intervals$.lower
upper.ci <- percentile_intervals$.upper - percentile_intervals$.estimate 
average.ci <- (lower.ci + upper.ci)/2
```


#Using t test to see if slope values overlap within 1sigma. Same as quadrature. 
```{r}
#Stick this into the bootstrap.linreg code-- unnested. 
  slopes.gcb <- sample_coefs$estimate
  slopes.gca <- sample_coefs$estimate
  slopes.gcc <- sample_coefs$estimate

#then do t-tests if you hate urself but you don't need to the results are the same as quadrature. 

t.test(ab_slopes_ft, a_slopes_ft)

gca.2.5 <- quantile(slopes.gca, .025)
gca.97.5 <- quantile(slopes.gca,.975)

gcc.2.5 <- quantile(slopes.gcc, .025)
gcc.97.5 <-  quantile(slopes.gcc, .975)

gcb.2.5 <- quantile(slopes.gcb, .025)
gcb.97.5 <-  quantile(slopes.gcb, .975)

```

Average % error
```{r}
percent.diff <- function(df) {
   ftd <- round(mean(abs(df$s.ft - df$db.ft)/((df$s.ft + df$db.ft)/2) * 100),2)
   fts <- round(2*sd(abs(df$s.ft - df$db.ft)/((df$s.ft + df$db.ft)/2) * 100),2)
 
    vd <- round(mean(abs(df$s.v - df$db.v)/((df$s.v + df$db.v)/2) * 100),2)
    vs <- round(2*sd(abs(df$s.v - df$db.v)/((df$s.v + df$db.v)/2) * 100),2)
  
    sad <- round(mean(abs(df$s.sa - df$db.sa)/((df$s.sa + df$db.sa)/2) * 100), 2)
    sas <- round(2*sd(abs(df$s.sa - df$db.sa)/((df$s.sa + df$db.sa)/2) * 100),2)
    esrd <- round(mean(abs(df$s.esr.ft - df$db.esr.ft)/((df$s.esr.ft + df$db.esr.ft)/2) * 100), 2)
    esrs <- round(2*sd(abs(df$s.esr.ft - df$db.esr.ft)/((df$s.esr.ft + df$db.esr.ft)/2) * 100),2)
  
    abs.diff <- c(ftd, vd, sad, esrd)
    twosigma <- c(fts, vs, sas, esrs)
    
return(data.frame(abs.diff, twosigma, row.names = c("ft", "vol", "sa", "esr")))
}

ri12 <- apatite %>% filter(ri == "1" | ri == "2")

percent.diff(apatite)
percent.diff(gcc)
percent.diff(ab)
percent.diff(ri12)
percent.diff(ri3)
```

#Sampling a subset of the data (without replacement)
```{r}
#Sample size should be ~10% of population 

unique2d <- matrix(nrow = 20, ncol = 1000)
unique3d <- matrix(nrow = 20, ncol = 1000)
uniqueslopes <- list()


for (i in 1:1000) {
  unique2d[,i] <- sample(common_apatite$s.ft, size=20, replace = F)
  unique3d[,i] <- sample(common_apatite$db.ft, size=20, replace = F)
  uniqueslopes[i] <- summary(lm(unique2d[,i] ~ 0 + unique3d[,i]))$coefficients[[1]]
}

uniqueslopes <- unlist(uniqueslopes) 
uniqueslopes <- as.data.frame(uniqueslopes)

unique2d <- as.data.frame(unique2d) %>%
  rename(simulated1 = V1, simulated2 = V2, simulated3 = V3, simulated4= V4)
unique2d <- unique2d %>% pivot_longer(cols = 1:4)

ggplot() + 
  geom_histogram(uniqueslopes, mapping = aes(x = uniqueslopes), fill = "white", color = "black") +
  labs(title = "Slopes calculated from 1000 samples of n= 20 (without replacement)", x = "Slopes") +
  geom_vline(xintercept = mean(uniqueslopes$uniqueslopes), color = "blue", size =1.5) +
  annotate("text", x = 1.045, y = 95, label = glue("mean = {round(mean(uniqueslopes$uniqueslopes),3)}"), size = 5)

sd(uniqueslopes$uniqueslopes)

ggplot() + 
  geom_histogram(unique2d, mapping=aes(x=value)) +
  facet_wrap(~name) +
  xlab("2D Ft") 

ggplot() + 
  geom_histogram(common_apatite, mapping=aes(x=s.ft)) 
```

#Sampling with replacement
```{r}
replaced2d <- matrix(nrow = 20, ncol = 1000)
replaced3d <- matrix(nrow = 20, ncol = 1000)
replacedslopes_ab <- list()
replacedslopes_c <- list()
for (i in 1:1000) {
  replaced2d[,i] <- sample(ab$s.v, size=20, replace = T)
  replaced3d[,i] <- sample(ab$db.v, size=20, replace = T)
  replacedslopes_ab[i] <- summary(lm(replaced2d[,i] ~ 0 + replaced3d[,i]))$coefficients[[1]]
}
replacedslopes_ab <- unlist(replacedslopes_ab)
replacedslopes_c <- unlist(replacedslopes_c)
replacedslopes_ <- as.data.frame(replacedslopes)

replaced2d <- as.data.frame(replaced2d) %>%
  rename(simulated1 = V1, simulated2 = V2, simulated3 = V3, simulated4= V4)
replaced2d <- replaced2d %>% pivot_longer(cols = 1:4)

ggplot() + 
  geom_histogram(replacedslopes, mapping = aes(x = replacedslopes), fill = "white", color = "black") +
  labs(title = "Slopes calculated from 1000 samples of n= 200 (with replacement)", x = "Slopes") +
  geom_vline(xintercept = mean(replacedslopes$replacedslopes), color = "red", size =1.5) +
  annotate("text", x = 1.035, y = 90, label = glue("mean = {round(mean(replacedslopes$replacedslopes),3)}"), size = 5)

sd(replacedslopes$replacedslopes)


mean(replacedslopes$replacedslopes)
ggplot() + 
  geom_histogram(replaced2d, mapping=aes(x=value)) +
  facet_wrap(~name) + xlab("2D Ft")


ggplot() + 
  geom_histogram(replaced2d, mapping=aes(x= replaced2d))




```

##Resources:

* [Bootstrapping Code](https://towardsdatascience.com/bootstrap-sampling-in-r-a7bc9d3ca14a)
* [Confidence Intervals vs Std Deviation](https://stats.stackexchange.com/questions/151541/confidence-intervals-vs-standard-deviation/377634#:~:text=standard%20deviation,-confidence%2Dinterval%20standard&text=The%2095%25%20confidence%20interval%20gives,a%20range%20of%20~95%25.)
* [Overlapping Confidence Intervals](https://towardsdatascience.com/tutorial-for-using-confidence-intervals-bootstrapping-860ba716aef3)


### Taylor Method

#Quadrature
```{r}
slope.uncertainty(c, "volume")

#read in spreadsheet of linear regression values
quad <- read_excel("./Linear Reg Results.xlsx", sheet="comp")

#function
error.propagation(quad, "common.only", "volume", "B", "C")
```

